import os
from openai import AsyncOpenAI
from dotenv import load_dotenv

load_dotenv()

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
if not OPENAI_API_KEY:
    raise EnvironmentError("âŒ OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")

client = AsyncOpenAI(api_key=OPENAI_API_KEY)

MAX_TOKENS_PER_CHUNK = 3000  # í† í° ë‹¨ìœ„ ê¸°ì¤€ (í”„ë¡¬í”„íŠ¸ + ì‘ë‹µ í¬í•¨)


async def generate_review_prompt_chunked(mr_desc, full_diff_text):
    chunks = split_text_by_token(full_diff_text, max_chars=3500)  # ë‹¨ìˆœ ë¬¸ì ê¸°ì¤€ ë¶„í• 
    responses = []

    for i, chunk in enumerate(chunks):
        print(f"ğŸ§  [OpenAI] GPT ìš”ì²­ {i+1}/{len(chunks)}...")
        prompt = f"""
[ğŸ“„ MR ì„¤ëª…]
{mr_desc}

[ğŸ”§ ë³€ê²½ ìš”ì•½ - Part {i+1}]
{chunk}

ì´ ë³€ê²½ì‚¬í•­ì„ ê¸°ë°˜ìœ¼ë¡œ ì½”ë“œ í’ˆì§ˆ, ë³´ì•ˆ, ì„±ëŠ¥, ë¦¬íŒ©í† ë§ ì¸¡ë©´ì—ì„œ ë¦¬ë·°ì–´ì²˜ëŸ¼ ë¦¬ë·°í•´ì¤˜.
"""
        res = await client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "ë‹¹ì‹ ì€ ìˆ™ë ¨ëœ ì½”ë“œ ë¦¬ë·°ì–´ì…ë‹ˆë‹¤."},
                {"role": "user", "content": prompt},
            ],
            temperature=0.7,
        )
        responses.append(res.choices[0].message.content.strip())

    return "\n\n".join(responses)


def split_text_by_token(text, max_chars=3500):
    # ë‹¨ìˆœíˆ ê¸¸ì´ ê¸°ì¤€ìœ¼ë¡œ ë¬¸ìì—´ ë¶„í• 
    chunks = []
    while text:
        chunks.append(text[:max_chars])
        text = text[max_chars:]
    return chunks
