import os
from openai import AsyncOpenAI
from dotenv import load_dotenv

load_dotenv()

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
if not OPENAI_API_KEY:
    raise EnvironmentError("âŒ OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")

client = AsyncOpenAI(api_key=OPENAI_API_KEY)

MAX_TOKENS_PER_CHUNK = 3000  # í† í° ë‹¨ìœ„ ê¸°ì¤€ (í”„ë¡¬í”„íŠ¸ + ì‘ë‹µ í¬í•¨)


async def generate_review_prompt_chunked(mr_desc, file_diff_text):
    # ì´ ë¬¸ìì—´ ê¸¸ì´ í™•ì¸
    total_len = len(file_diff_text)
    print(f"ğŸ’¾ [OpenAI] íŒŒì¼ diff ê¸¸ì´: {total_len} ë¬¸ì")
    
    # diffê°€ ë„ˆë¬´ ê¸¸ë©´ ë©”ì‹œì§€ ë¶„í•  ì—¬ë¶€ ê²°ì •
    if total_len > 6000:  # OpenAI APIì˜ í† í° ì œí•œì„ ê³ ë ¤í•œ ê°’
        print("ğŸ”„ [OpenAI] ê¸¸ì´ ì œí•œìœ¼ë¡œ ë©”ì‹œì§€ ë¶„í• ")
        chunks = split_text_by_token(file_diff_text, max_chars=5500)
        responses = []

        for i, chunk in enumerate(chunks):
            print(f"ğŸ§  [OpenAI] GPT ìš”ì²­ {i+1}/{len(chunks)}...")
            prompt = f"""
[ğŸ“„ MR ì„¤ëª…]
{mr_desc}

[ğŸ”§ íŒŒì¼ ë³€ê²½ ì‚¬í•­ - Part {i+1}/{len(chunks)}]
{chunk}

ì´ ë³€ê²½ì‚¬í•­ì„ ê¸°ë°˜ìœ¼ë¡œ ì½”ë“œ í’ˆì§ˆ, ë³´ì•ˆ, ì„±ëŠ¥, ë¦¬íŒ©í† ë§ ì¸¡ë©´ì—ì„œ êµ¬ì²´ì ìœ¼ë¡œ ë¦¬ë·°í•´ì£¼ì„¸ìš”. íŒŒì¼ì— ì í•©í•œ ë°©ì‹ìœ¼ë¡œ ë¦¬ë·°í•´ì£¼ì„¸ìš”.
"""
            res = await client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ ìˆ™ë ¨ëœ ì½”ë“œ ë¦¬ë·°ì–´ì…ë‹ˆë‹¤. êµ¬ì²´ì ì´ê³  ì•¡ì…˜ê°€ëŠ¥í•œ ì½”ë“œ ë¦¬ë·°ë¥¼ ì œê³µí•©ë‹ˆë‹¤."},
                    {"role": "user", "content": prompt},
                ],
                temperature=0.7,
            )
            responses.append(res.choices[0].message.content.strip())
        
        return "\n\n".join(responses)
    else:
        print(f"ğŸ§  [OpenAI] ë‹¨ì¼ GPT ìš”ì²­...")
        prompt = f"""
[ğŸ“„ MR ì„¤ëª…]
{mr_desc}

[ğŸ”§ íŒŒì¼ ë³€ê²½ ì‚¬í•­]
{file_diff_text}

ì´ ë³€ê²½ì‚¬í•­ì„ ê¸°ë°˜ìœ¼ë¡œ ì½”ë“œ í’ˆì§ˆ, ë³´ì•ˆ, ì„±ëŠ¥, ë¦¬íŒ©í† ë§ ì¸¡ë©´ì—ì„œ êµ¬ì²´ì ìœ¼ë¡œ ë¦¬ë·°í•´ì£¼ì„¸ìš”. ë¶ˆí•„ìš”í•œ ë‚´ìš©ì€ ì œì™¸í•˜ê³  ì¤‘ìš”í•œ í•´ê²°ë°©ì•ˆì— ì´ˆì ì„ ë§ì¶”ì„¸ìš”.
"""
        res = await client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "ë‹¹ì‹ ì€ ìˆ™ë ¨ëœ ì½”ë“œ ë¦¬ë·°ì–´ì…ë‹ˆë‹¤. êµ¬ì²´ì ì´ê³  ì•¡ì…˜ê°€ëŠ¥í•œ ì½”ë“œ ë¦¬ë·°ë¥¼ ì œê³µí•©ë‹ˆë‹¤."},
                {"role": "user", "content": prompt},
            ],
            temperature=0.7,
        )
        return res.choices[0].message.content.strip()


def split_text_by_token(text, max_chars=3500):
    # ë‹¨ìˆœíˆ ê¸¸ì´ ê¸°ì¤€ìœ¼ë¡œ ë¬¸ìì—´ ë¶„í• 
    chunks = []
    while text:
        chunks.append(text[:max_chars])
        text = text[max_chars:]
    return chunks
