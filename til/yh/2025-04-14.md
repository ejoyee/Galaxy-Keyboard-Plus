# MCP 기반 민감정보 마스킹 코드 도구 개발

작성자: 영환 최

### 전체 내용

## 1. 필요한 핵심 기술과 도구

- **코드 분석 및 파싱 도구:** 소스 코드를 파싱하고 민감 정보를 식별하기 위해 AST 기반 코드 파서(예: Python의 `ast` 모듈, JavaScript의 Babel 등)나 정적 분석기가 필요하다. 이러한 도구를 통해 변수명, 리터럴, 주석 등에서 민감한 내용을 추출할 수 있다. 또한 API 키나 비밀 값 탐지를 위해 TruffleHog, Gitleaks 같은 **시크릿 스캐너**를 활용할 수도 있다.
- **민감 정보 치환 알고리즘:** 식별된 민감 정보를 안전한 **플레이스홀더(placeholder)**로 대체하고, 나중에 원복할 수 있도록 매핑 테이블을 관리하는 로직이 필요하다. 예를 들어 `<APIKEY_1>`, `<VAR_1>` 등의 토큰으로 치환하고, 이를 원래 값과 매핑해 저장해두는 방식이다[cadosecurity.com](https://www.cadosecurity.com/blog/introducing-masked-ai-an-open-source-library-that-enables-the-usage-of-llm-apis-more-securely#:~:text=1,a%20placeholder%20in%20its%20place). 이러한 기능은 Cado Security의 **Masked-AI** 라이브러리처럼 오픈소스로 구현된 사례도 있다[cadosecurity.com](https://www.cadosecurity.com/blog/introducing-masked-ai-an-open-source-library-that-enables-the-usage-of-llm-apis-more-securely#:~:text=1,a%20placeholder%20in%20its%20place).
- **LLM 연동 및 API:** OpenAI의 GPT-4/GPT-3.5, Anthropic Claude 등 **대형 언어 모델(LLM)** API와 통신할 수 있는 SDK나 HTTP 클라이언트가 필요하다. 예를 들어 Python용 OpenAI API 라이브러리, 또는 Anthropic API, HuggingFace Hub 등을 사용할 수 있다. 또한 프롬프트 구성 및 토큰 관리 기능도 포함된다.
- **MCP 서버 및 SDK:** **Model Context Protocol (MCP)**은 LLM 기반 애플리케이션과 외부 도구(서버) 간 통신 표준이다[medium.com](https://medium.com/data-and-beyond/mcp-servers-a-comprehensive-guide-another-way-to-explain-67c2fa58f650#:~:text=MCP%20stands%20for%20Model%20Context,through%20a%20standardized%20interface). Anthropic 등에서 제공하는 공식 **MCP SDK**(Python, TypeScript 등 지원)를 이용하면 JSON-RPC 기반 통신, SSE(서버발송 이벤트) 스트리밍, 인증 등 기본 기능을 쉽게 구현할 수 있다. SDK를 사용하면 표준 프로토콜 준수를 보장하고, 보일러플레이트 코드를 줄일 수 있다. 대신 SDK 없이 직접 구현하려면 JSON-RPC 메시지 처리, 규약 준수 등의 작업을 수작업으로 처리해야 한다.
- **기타 개발 도구:** 버전 관리(git)와 테스트 프레임워크를 통한 코드 무결성 검사, 로깅 및 모니터링 도구(민감 정보 유출이 없도록 로그 필터링 적용), 그리고 VS Code와 같은 IDE와의 연동(예: MCP 클라이언트를 통합한 확장) 등도 고려된다.

## 2. 단계별 개발 플로우

예를 들어 **Masked-AI** 라이브러리의 동작 플로우를 보면, 애플리케이션에서 들어온 프롬프트 내 **이름** “Adam”과 **IP 주소** “8.8.8.8”을 각각 `<Name1>`과 `<IP_1>`로 마스킹한 후 LLM에 보내고, 응답에서 해당 토큰들을 다시 원래 값으로 복원한다. 이처럼 **민감 코드 처리 도구**도 일련의 단계를 거쳐 동작한다:

1. **전처리 – 민감 정보 식별 및 마스킹:** 사용자가 작성한 원본 코드를 받아 우선 정적 분석이나 패턴 매칭으로 **민감한 부분을 탐지**한다. 예를 들어 API 키 패턴(`AKIA...` 등), 비밀키 시작/끝 마커(`----BEGIN PRIVATE KEY-----` 등), 혹은 회사 이름/프로젝트 코드명이 포함된 변수명 등을 찾는다. 식별된 민감 정보는 고유한 플레이스홀더로 **치환**된다. 치환 시에는 각 항목에 일대일 대응되는 토큰을 부여하고, 이를 **룩업 테이블**에 저장해둔다[cadosecurity.com](https://www.cadosecurity.com/blog/introducing-masked-ai-an-open-source-library-that-enables-the-usage-of-llm-apis-more-securely#:~:text=1,a%20placeholder%20in%20its%20place). 치환된 코드의 문법이나 기능이 유지되도록, 토큰 문자열은 코드에서 유효한 식별자 형태로 선택한다 (예: 공백이나 특수문자 제외). 이 단계에서는 **데이터 최소화** 원칙에 따라 필요 이상으로 상세한 정보는 모두 마스킹되어, LLM에는 **업무 핵심 로직만 전달**되게 한다[huggingface.co](https://huggingface.co/blog/lynn-mikami/mcp-servers#:~:text=,communications%20with%20proper%20certificate%20validation).
2. **MCP 서버 상호작용 – LLM 요청 생성:** 마스킹된 코드를 MCP 서버 모듈이 받아서, 원하는 작업에 대한 **LLM 프롬프트**를 구성한다. MCP 서버는 이 기능을 하나의 **툴(tool) 또는 메서드**로 노출할 수 있다. 예를 들어 `analyze_code`라는 툴 메서드를 만들고, 매개변수로 코드와 원하는 작업 타입(에러검출, 리팩토링 등)을 받도록 한다. 서버는 입력으로 받은 마스킹 코드와 작업 지시를 기반으로 LLM에 보낼 프롬프트를 생성한다. 이때 시스템 프롬프트 등을 활용하여 LLM에게 “코드 내 특정 토큰들은 민감정보를 가린 것이니, 로직 개선에 활용하되 토큰 자체는 변경하지 말 것”과 같은 지침을 줄 수 있다. 준비된 프롬프트를 가지고 서버는 **LLM API 호출**을 수행한다. 호출은 HTTP 요청으로 외부 GPT/Claude 서비스에 보내거나, 자체 호스팅된 모델이 있다면 로컬 inference를 할 수도 있다. 요청 시 토큰 수나 모델 설정(max tokens, temperature 등)을 작업 종류에 맞게 조정한다.
    
    MCP 서버는 이러한 LLM 요청 과정을 내부적으로 처리하지만, **MCP 프로토콜**을 통해 클라이언트(예: IDE 내 AI 에이전트)에는 해당 작업이 진행됨을 나타내는 진행 상태나 부분 결과를 전송할 수 있다. MCP는 Server-Sent Events 기반 **스트리밍 응답**을 지원하므로, LLM의 토큰 생성 결과를 실시간으로 클라이언트에 흘려보내는 것도 가능하다[github.com](https://github.com/cyanheads/model-context-protocol-resources/blob/main/guides/mcp-server-development-guide.md#:~:text=Streaming%20Responses). 예컨대 리팩토링된 코드를 한 줄씩 스트리밍하면 사용자 경험이 개선된다.
    
3. **LLM 응답 처리 – 후처리 및 복원:** LLM으로부터 마스킹된 코드에 대한 처리 결과(응답 코드, 설명 등)가 도착하면 MCP 서버는 이를 파싱한다. 응답 내용에서 앞서 치환했던 플레이스홀더 토큰들을 찾아 **원래 값으로 복원(post-processing)** 한다[cadosecurity.com](https://www.cadosecurity.com/blog/introducing-masked-ai-an-open-source-library-that-enables-the-usage-of-llm-apis-more-securely#:~:text=1,a%20placeholder%20in%20its%20place). 룩업 테이블을 참조하여 `<VAR_3>` → 실제 변수명, `<APIKEY_1>` → 실제 키 값 식으로 **역치환**을 수행한다. 복원이 끝난 최종 결과물은 사용자가 처음 작성한 코드와 동일한 컨텍스트를 가진 상태가 된다. 마지막으로 MCP 서버는 복원된 코드를 클라이언트나 사용자에게 반환한다. 이 결과는 IDE 편집기에 적용되거나, 사용자에게 새 코드 제안(diff) 형태로 제시될 수 있다.
    
    반환 전후로 MCP 서버는 민감 정보가 다시 드러난 최종 결과를 취급하므로 **메모리 상에서 빠르게 소멸**시키고, 로그에도 남기지 않는 등 보안을 유지해야 한다. 또한 혹시 모를 오류에 대비해, 복원된 코드의 **문법 검증**이나 **테스트 수행**을 자동화하여 LLM 변경으로 인한 버그가 없는지 확인하는 절차를 추가로 둘 수도 있다.
    

## 3. 민감 정보 치환 전략

**무엇을 어떻게 치환할 것인지**에 대한 전략을 세우는 것이 이 시스템의 핵심이다. 다음과 같은 실용적 방안을 고려할 수 있다:

- **정의된 패턴 기반 치환:** 우선 노출되면 안 되는 비밀 정보의 패턴을 정의한다. 예컨대 API 키(정해진 prefix와 길이를 가진 문자열), 비밀 키 블럭, 데이터베이스 비밀번호, 토큰 등의 **정규표현식 패턴 목록**을 준비해 코드에서 검색한다. 찾은 값들은 `<SECRET_1>`, `<SECRET_2>` 등으로 치환한다. 비즈니스 도메인에 특화된 값(예: 회사 이름, 프로젝트 코드명)도 리스트로 관리하여 포함 시 매스킹한다. Salesforce의 Einstein Trust Layer도 PII나 결제정보 등을 프롬프트에 넣기 전에 자동 **마스킹**하여 제3자 LLM에 노출되지 않도록 한다[salesforce.com](https://www.salesforce.com/artificial-intelligence/trusted-ai/#:~:text=Data%20Masking). 이러한 사전 정의 패턴으로 빠르고 일괄적인 치환이 가능하다.
- **정적 코드 분석 활용:** 코드의 구조를 인지하기 위해 **AST 기반 정적 분석**을 적용한다. 이를 통해 단순 문자열 매칭 이상의 정확도를 얻을 수 있다. 예를 들어, 함수/클래스/변수 이름에서 **도메인 특화 키워드**(예: `CustomerData`, `PaymentInternal` 등)가 포함된 경우 치환하거나, 문자열 리터럴 중 URL, 경로, 이메일 주소 등의 민감 데이터를 식별하여 치환한다. 정적 분석을 쓰면 **코드 문맥을 보존**하면서 필요한 토큰만 바꿀 수 있다. 특히 변수명 치환의 경우, 코드 전체에서 해당 이름을 동일 토큰으로 치환해야 하므로 AST로 심볼 테이블을 만들어 일괄 처리하면 consistency를 유지할 수 있다. 치환 기준은 **“업무 맥락을 유추할 수 있는지”** 여부로 결정한다. 예를 들어 단순 Loop index인 `i, j` 등은 민감하지 않지만, `AcmeCorpClient` 같은 이름은 회사명을 드러내므로 마스킹한다. 치환 후에도 코드가 **컴파일 가능한 상태**로 유지되어야 하기 때문에, 언어별로 예약어 충돌이 없는 토큰명을 사용하고, 자료형이나 포맷이 중요한 곳(예: 정규식 패턴 내 문자열)에서는 길이나 형태를 가급적 비슷하게 맞춰준다.
- **딥러닝 기반 탐지 보조:** 정규식이나 AST로 놓칠 수 있는 미묘한 민감정보는 **ML 모델**의 도움을 받을 수 있다. 예를 들어 사전훈련된 NER(개체명 인식) 모델이나 소형 LLM을 **로컬에서 실행**하여 코드 주석이나 식별자 중 민감도 높은 표현을 찾아내는 것이다[elastic.co](https://www.elastic.co/search-labs/blog/rag-security-masking-pii#:~:text=Identifying%20and%20masking%20PII%20and,on%20your%20needs%20and%20testing). Elastic 연구에서는 경량 LLM인 Mistral-7B를 로컬에서 구동해, OpenAI에 보내기 전에 **질문/문서 내 PII를 마스킹**하는 실험을 했다[elastic.co](https://www.elastic.co/search-labs/blog/rag-security-masking-pii#:~:text=the%20work%20of%20Masking%20before,data%20to%20a%20public%20LLM). 이러한 접근은 정해진 패턴 외의 사례(예: 신규 포맷의 키, 내부 용어 등)도 학습된 확률로 잡아낼 수 있다는 장점이 있다. 다만 과대/과소 탐지에 대한 검증이 필요하며, 모델 구동 비용이 추가 발생한다. **프라이버시 vs 성능 트레이드오프**를 고려해, 핵심은 정적 규칙 기반으로 처리하되 보조적으로 ML을 활용하는 전략이 현실적이다.
- **치환 토큰 설계:** 치환에 사용하는 플레이스홀더 토큰은 **고유하고 충돌이 없어야** 한다. 예를 들어 `<MASK1>` 형태로 모두 치환하면, LLM이 처리중에 `<MASK10>`을 `<MASK1`+`0`으로 오인하는 등의 문제도 있을 수 있으므로 구분자를 명확히 한다. `<SECRET_###>`처럼 범주+번호 조합으로 만들면 여러 종류의 민감정보가 섞여 있어도 구별된다 (`<APIKEY_1>`, `<NAME_1>`, `<URL_1>` 등). 다만 이러한 이름 자체가 원래 값의 성격을 어느 정도 노출할 수 있으므로, 매우 민감한 경우 모두 `<MASK_1>`, `<MASK_2>`로만 두고 내부적으로만 카테고리를 기록하는 방법도 있다. 여러 파일이나 세션에 걸쳐 치환해야 한다면 **UUID** 같은 고유키를 써서 전역 유일성을 보장할 수도 있다. 중요한 것은 동일한 민감항목에 항상 동일 토큰이 대응되어, LLM이 **맥락을 일관되게 이해**하도록 하는 것이다. 예를 들어 같은 API 키가 코드 여러 곳에 등장하면 전부 `<APIKEY_1>`로 동일하게 치환해야 LLM이 그것들을 같은 값으로 인지하고 적절한 처리를 할 수 있다.
- **매핑 테이블 관리:** 치환한 원본-토큰 쌍은 안전한 저장소에 보관한다. 일반적으로는 메모리 내 딕셔너리 형태로 유지하고, 요청 처리 완료 후 폐기한다[cadosecurity.com](https://www.cadosecurity.com/blog/introducing-masked-ai-an-open-source-library-that-enables-the-usage-of-llm-apis-more-securely#:~:text=1,a%20placeholder%20in%20its%20place). 만약 장시간에 걸친 세션(예: 사용자가 연속해서 리팩토링을 여러 번 수행)이라면 세션별 매핑 테이블을 유지하여 이전에 치환된 항목이 일관되게 유지되도록 한다. 이 테이블은 **절대 외부로 유출되면 안되므로**, 디스크에 쓸 경우 암호화하거나, 로그에 남기지 않으며, 최소 권한으로 취급한다[huggingface.co](https://huggingface.co/blog/lynn-mikami/mcp-servers#:~:text=,communications%20with%20proper%20certificate%20validation).

이러한 치환 전략을 통해 **프롬프트 내 민감정보 노출을 최소화**하면서도, LLM이 코드 맥락을 충분히 이해할 수 있게 균형을 잡는 것이 목표다[salesforce.com](https://www.salesforce.com/artificial-intelligence/trusted-ai/#:~:text=context%20about%20your%20business%2C%20while,party%20LLM)[salesforce.com](https://www.salesforce.com/artificial-intelligence/trusted-ai/#:~:text=Data%20Masking). 또한 도메인 지식이 쌓일수록 치환 규칙을 지속 보강하여 정확성을 높여야 한다[elastic.co](https://www.elastic.co/search-labs/blog/rag-security-masking-pii#:~:text=Identifying%20and%20masking%20PII%20and,on%20your%20needs%20and%20testing).

## 4. LLM 응답 품질 및 민감 정보 복원 관련 문제점과 대응

**민감정보 마스킹**을 도입하면 얻는 이점이 크지만, LLM을 활용한 코드 생성/수정 품질과 결과 복원 과정에서 몇 가지 도전 과제가 발생할 수 있다:

- **(a) LLM 응답 품질 저하 우려:** 코드에서 의미 있는 식별자나 문자열이 모두 `<MASK_x>` 형태로 치환되면, 모델이 문맥을 이해하는 데 어려움을 겪을 수 있다. 예를 들어 변수명이 `customerEmail`이었다면 역할을 짐작할 수 있지만 `<VAR_12>`로 바뀌면 의미가 사라진다. 그로 인해 생성된 코드의 정합성이 떨어지거나, 엉뚱한 수정이 제안될 가능성이 있다. **대응:** 프롬프트에 **추가 지시**를 넣어 모델이 토큰을 어떻게 취급해야 하는지 알려준다. 예: `"코드 내 MASK 토큰들은 각기 다른 민감 데이터를 나타냅니다. 이들을 변경하거나 제거하지 말고, 의미상 같은 변수로 취급하세요."`와 같은 시스템 메시지를 함께 보낸다. 또한 토큰명에 약간의 힌트를 남기는 방법도 있다. 예를 들어 `<CLASS_1>`처럼 대문자로 시작하면 클래스명 자리임을 암시할 수 있다. 그러나 이런 힌트는 민감정보를 유추할 수 없도록 아주 일반적으로 해야 한다. 결국 충분한 테스트를 통해, 치환 수준이 LLM 답변 품질에 큰 영향을 주는지 점검해야 한다. 필요하다면 일부 **맥락 정보를 비민감 형태로 제공**하는 것도 방법이다 (예: “<VAR_12>는 고객 이메일 주소를 나타냅니다” 같은 힌트를 주면 품질은 올라가나, 이조차 노출이 꺼려진다면 생략).
- **(b) LLM의 플레이스홀더 변형 문제:** LLM이 응답을 생성하는 중에 실수로 플레이스홀더를 변경하거나 없애버리는 경우가 있다. 이상적으로는 모델이 `<MASK_1>` 등을 그대로 보존해야 하지만, 모델이 코드를 “개선”하면서 해당 토큰의 이름을 바꾸거나 통째로 제거할 가능성도 있다. 예를 들어, 사용되지 않는 변수라고 판단하여 `<VAR_5>` 토큰이 들어간 줄을 삭제해버릴 수 있다. 혹은 `<VAR_5>`와 `<VAR_6>`가 비슷해 보여 하나로 합쳐버리는 등 예기치 않은 변형을 할 수도 있다. **대응:** 우선 위의 프롬프트 지시를 통해 **토큰을 변경하지 말 것**을 강조한다. 그래도 사라질 수 있으므로, **후처리 단계에서 검증 로직**을 둔다. LLM 응답이 오면, 원본 코드에 존재하던 모든 `<MASK_x>` 토큰이 응답에도 존재하는지 비교한다. 만약 일부 토큰이 누락되었다면, 해당 위치에 원본 민감 데이터를 복원할 수 없게 되므로 **사용자에게 경고**하거나, 필요시 원본 코드를 해당 부분만 그대로 유지하도록 **응답을 보정**한다. (예: LLM이 `<APIKEY_1>`이 쓰인 줄을 삭제했다면, 결과 코드를 적용할 때 그 줄은 원본 코드에서 복사하여 유지시킴). 반대로 LLM이 새로운 `<MASK>` 토큰을 만들어냈다면 (예: 새로운 민감정보가 필요한 API 호출 코드를 추가하면서 placeholder를 끼워넣은 경우), 이는 매핑에 없는 항목이다. 이런 경우 새 토큰을 그대로 둘지, 아니면 사용자가 나중에 수동으로 실제 값을 채워넣도록 안내할지 결정해야 한다. 보통은 **신규 토큰은 그대로 둔 채** 사용자가 그 부분을 채우도록 하는 편이 안전하다 (시스템이 추측으로 채우면 오히려 실제 민감정보와 mismatch될 수 있다).
- **(c) 코드 논리 훼손 및 복원 오류:** LLM이 리팩토링이나 최적화를 하다가 코드의 일부를 변경하면, 치환했던 민감 데이터를 복원한 후 **논리나 기능이 깨질 위험**이 있다. 예를 들어, LLM이 `<URL_1>` 토큰 (원래는 특정 서버 URL)과 문자열을 연결하는 코드를 단순화하면서 토큰을 빼먹었다면, 복원 후 실제 URL이 누락되어 버그가 생길 수 있다. 또는 `<PASSWORD_1>`을 하드코딩하지 말라고 삭제했는데 복원 후에는 비밀번호 설정이 사라져 동작이 바뀔 수 있다. **대응:** 이러한 상황을 막기 위해 **테스트 코드**를 함께 생성하고 실행해보는 방법이 있다. MCP 서버가 LLM에게 리팩토링을 시키는 동시에 해당 코드에 대한 테스트 케이스 생성을 요청하고, 복원 후 테스트를 돌려봄으로써 주요 기능이 유지되는지 확인할 수 있다. 혹은 최소한 **구문 오류 검증**(컴파일/파싱 테스트)을 자동화하여, LLM 응답 적용 후 코드가 구문 오류 없이 잘 복원되었는지 체크한다. 만약 문제를 발견하면, 해당 응답을 신뢰하지 않고 사용자에게 수정을 권유하거나, LLM에게 재요청할 수 있다.
- **(d) 새로운 민감정보 요소에 대한 처리:** LLM이 코드를 확장하거나 기능을 추가하면서 **새로운 구성요소**(예: 새로운 설정값, 새로운 클래스 등)를 만들어낼 수 있다. 이 때 원래 코드에 없던 민감정보(예: 새 API 키 변수)를 요구할 수 있는데, 당연히 LLM은 placeholder로만 제시할 수 있다. 예를 들어 데이터베이스 연결 코드를 추가하면서 `<DB_PASSWORD_NEW>`를 넣어 반환하면, 사용자는 그 부분을 실제 값으로 채워넣어야 할 것이다. **대응:** 시스템이 이러한 새 민감요소를 인지하고 사용자에게 알려주어야 한다. “여기에 들어갈 실제 비밀 값을 입력하세요”라고 안내하거나, 만약 기업 내부적으로 이미 알고 있는 값이면 자동 채워넣을 수도 있다. 그러나 **기본 방침은 사용자가 최종 결정하도록** 하는 것이 안전하다. 또한 이러한 새로 추가된 부분은 향후 동일 과정에서 다시 마스킹 대상이 되므로, 시스템은 **매핑 테이블을 업데이트**하여 다음번 처리 시 인지할 수 있게 한다.
- **(e) 민감정보 유출 위험 감소 효과:** 한편으로, 이 시스템 덕분에 LLM 응답 자체에는 원본 민감정보가 포함되지 않으므로, LLM이 답변 중에 그 정보를 노출할 위험은 원천 차단된다. 예컨대 과거에는 ChatGPT 등에 소스코드를 보내면 응답에 API 키가 그대로 재노출될 수 있었지만, 이제는 `<APIKEY_1>` 같은 토큰만 보이므로 이러한 **데이터 유출 위험은 거의 0에 수렴**한다. 다만 토큰이 너무 단순하면 혹시 모델이 해당 토큰을 일반 단어로 오인해 변환해버리는 경우도 생각해야 한다. (예: `<PI_1>`를 보고 “3.14”로 바꿔버리는 등은 이론상 가능) 따라서 인위적인 문자열 형태(`<` `>` 포함 등)를 사용하여 모델이 임의 조작하지 않도록 설계한다.
- **(f) 사용자 경험 측면:** 이상적인 경우, 사용자는 자신이 작성한 민감 코드 조각을 직접 LLM에 보내지 않고도 동일한 편의를 누리게 된다. 다만 응답 결과에서 placeholder가 원복되어 나오므로, **사용자는 마치 LLM이 원본 코드를 직접 이해하고 답한 것처럼 결과를 얻는다.** 이 과정에서 시스템이 하는 일은 투명하게 숨겨지므로 사용자 경험에는 큰 차이가 없다. 혹여 복원 단계에서 일부 놓친 부분이나 수정 필요한 부분이 있다면, 친절한 메시지로 알려주는 것이 좋다. 예: *“일부 보안 관련 값은 실제 값으로 치환되지 않았습니다. 해당 부분을 수동으로 확인해주세요.”*

요약하면, 치환→LLM→복원의 **연계 작업을 얼마나 견고하게 만드느냐**가 관건이다. 이를 위해서는 LLM 프롬프트 설계, 사후 검증 절차, 오류시 롤백 전략 등을 마련해 두어, 품질 저하와 기능 오류를 최소화해야 한다.

## 5. MCP 서버 직접 제작 vs 기존 MCP SDK 활용 장단점

시스템을 구현할 때 **MCP 서버를 처음부터 직접 개발**할지, 아니면 Anthropic 등에서 제공하는 **기존 MCP SDK를 활용**할지 선택해야 한다. 두 접근의 장단점을 비교하면 다음과 같다:

- **기존 MCP SDK 활용:**
    - *장점:* 표준 프로토콜 구현을 **신뢰성 있게 빠르게** 구축할 수 있다. JSON-RPC 통신, 서버-클라이언트 핸드쉐이크, 에러 처리 등 많은 공통 기능이 SDK에 이미 구현되어 있어 개발 시간이 단축된다. 예를 들어 Anthropic의 공식 SDK는 **필드 마스킹, 레이트 리밋, 보안 헤더** 등도 내장하고 있어[github.com](https://github.com/jmcentire/mcp#:~:text=Kotlin%2C%20Rust,for%20installation%2C%20usage%2C%20and%20customization), 민감정보 치환 같은 보안 요구사항도 설정으로 지원될 수 있다. 또한 SDK 사용 시, Claude Desktop이나 VSCode Extensions 등 **MCP 클라이언트 표준**을 따르는 도구들과 호환성이 보장된다. 업데이트도 SDK 측에서 따라가기 때문에, MCP 프로토콜에 변화가 생기면 비교적 쉽게 대응 가능하다.
    - *단점:* SDK가 제공하는 구조와 패턴에 **제약**을 따라야 한다. 커스텀한 통신 시나리오나 최적화가 어렵다면 성능 튜닝에 한계가 있을 수 있다. 예를 들어 매우 특수한 스트리밍 처리나 비표준 기능을 원할 경우 SDK 레벨에서 지원되지 않으면 구현이 까다롭다. 또한 외부 SDK에 대한 **러닝 커브**가 존재하며, 의존성 관리나 SDK 라이선스도 고려해야 한다. 성능 면에서는 SDK 자체 오버헤드는 크지 않겠지만, 불필요한 기능까지 포함될 경우 약간의 메모리/CPU 부담이 있을 수 있다.
- **MCP 서버 직접 제작 (커스텀 구현):**
    - *장점:* **유연성과 경량화**가 가능하다. 자신의 용도에 딱 맞게 필요한 기능만 구현하므로, 구조를 단순화하고 불필요한 부분을 없앨 수 있다. 성능 최적화도 손쉽다 – 예를 들어 특정 전처리/후처리 과정을 프로토콜 단계에 맞춰 특수 처리하거나, 자체 캐싱 메커니즘을 넣는 등 자유도가 높다. 또한 MCP 자체를 변형/확장해서 사내 전용 프로토콜처럼 활용하는 것도 가능하다 (다만 이것은 MCP 표준 호환성을 포기하는 대가가 있다). 요컨대 **프로토콜에 대한 완전한 통제권**을 갖게 된다.
    - *단점:* MCP 표준의 세부사항(예: JSON-RPC 2.0, SSE 스트리밍, 에러 코드, Capabilities 정의 등)을 모두 직접 구현해야 하므로 **개발 난이도와 비용이 증가**한다. 잘못 구현하면 표준과 어긋나서 다른 MCP 호환 클라이언트에서 인식 못 하거나, 미묘한 버그(에이전트가 툴을 못찾는다든지)를 유발할 수 있다. 특히 인증, 권한, 스Chemavalidation 등도 직접 처리해야 하므로 보안 취약점 위험도 커질 수 있다[huggingface.co](https://huggingface.co/blog/lynn-mikami/mcp-servers#:~:text=MCP%20server%20security%20represents%20a,for%20production%20MCP%20server%20deployments)[huggingface.co](https://huggingface.co/blog/lynn-mikami/mcp-servers#:~:text=,communications%20with%20proper%20certificate%20validation). 유지보수 측면에서도 MCP 표준이 발전할 때마다 수동으로 개선해야 한다.
    - *적정선:* 현실적으로 초기에는 SDK로 빨리 개발한 뒤, 병목이나 한계가 뚜렷해지면 일부 컴포넌트를 교체하거나 특정 기능만 custom 하기도 한다. Twilio의 사례에 따르면, 많은 개발자들이 **초기엔 MCP의 편의성**으로 시작했다가 나중에 정확히 필요한 툴 통합만 custom으로 전환하는 경향이 있다고 한다[twilio.com](https://www.twilio.com/en-us/blog/twilio-alpha-mcp-server-real-world-performance#:~:text=with%20MCP,exactly%20what%20their%20agent%20needs). 우리 시나리오에서도 우선 SDK로 기반을 만들고 치환/LLM 호출 로직을 넣은 뒤, 필요하면 성능 중요한 경로만 직접 최적화하는 접근이 바람직하다.
- **SDK의 종류:** Anthropic 이외에도 커뮤니티 주도로 다양한 MCP 서버 템플릿과 SDK가 등장하고 있다. Python, TypeScript, Kotlin, Rust 등 언어별 구현체가 존재하며[github.com](https://github.com/jmcentire/mcp#:~:text=,for%20installation%2C%20usage%2C%20and%20customization), 예컨대 OpenAI도 자사 에이전트용 MCP 호환 인터페이스를 발표하였다[twilio.com](https://www.twilio.com/en-us/blog/twilio-alpha-mcp-server-real-world-performance#:~:text=Protocol%20%28MCP%29%20for%20AI%20agents,out%20our%20introductory%20blog%20post). 따라서 익숙한 언어의 SDK를 골라 사용하면 개발 생산성이 올라갈 것이다. 반대로 사내 요구사항으로 MCP 프로토콜을 살짝 변형해야 한다면 (예: 사내 보안 규격에 맞춘 통신) 직접 구현이 불가피할 수 있다.

**종합적으로**, 표준 SDK 활용은 개발을 빠르게 하고 검증된 베스트프랙티스를 얻는 방법이고, 직접 제작은 커스터마이징과 경량화의 길이다. 요구사항과 리소스를 고려하여 결정하되, **장기적 유지보수와 호환성** 측면에서 SDK 활용 쪽이 유리한 경우가 많다.

## 6. MCP 기반 아키텍처 성능 및 사용자 경험

마지막으로, **치환 + LLM 호출 + 복원**으로 이어지는 MCP 기반 도구의 성능을 살펴보면 다음과 같은 평가와 전망을 할 수 있다:

- **응답 속도 분석:** 일반적으로 **가장 큰 응답 지연 요인**은 LLM API 호출 시간이다. 외부 GPT-4 모델을 호출하면 수 초에서 수십 초까지 걸릴 수 있고, GPT-3.5나 작은 모델은 좀 더 빠르다. 여기에 비해 전처리(치환)와 후처리(복원)는 문자열 처리 및 파싱 작업으로, 코드 길이가 아주 길지 않은 한 **수백 밀리초 이내**(대부분는 수십 ms 수준)에 완료된다. MCP 프로토콜 통신 자체의 오버헤드도 경미하다 – 로컬에서 동작 시 IPC 혹은 localhost HTTP로 통신하므로 왕복 지연이 몇 ms 수준이며, JSON 직렬화/역직렬화 비용도 미미하다. 실제 Medium의 MCP 해설에 따르면 MCP는 **프로토콜이 경량**하여 오버헤드가 거의 없고 스트리밍 지원으로 지연을 줄일 수 있다고 한다 (MCP 서버는 고성능으로 만들기 쉽다고 평가됨)[medium.com](https://medium.com/data-and-beyond/mcp-servers-a-comprehensive-guide-another-way-to-explain-67c2fa58f650#:~:text=In%20summary%2C%20MCP%20servers%20can,they%20support%20streaming%20and). 따라서 **전체 지연의 대부분은 LLM 처리 시간**으로 볼 수 있다.
- **스트리밍으로 UX 개선:** MCP의 스트리밍 기능을 적극 활용하면, LLM의 토큰 생성 즉시 결과를 순차적으로 사용자에게 보여줄 수 있다[github.com](https://github.com/cyanheads/model-context-protocol-resources/blob/main/guides/mcp-server-development-guide.md#:~:text=Streaming%20Responses). 예컨대 코드 리뷰 결과를 한 줄씩 출력하거나, 리팩토링된 새 코드를 부분부분 실시간으로 편집기에 반영하면 사용자 체감 속도가 향상된다. 최종 완성본은 LLM 처리 완료 후 얻어지지만, **진행 상황을 가시화**하면 긴 응답도 덜 지루하게 느껴진다. 반면 스트리밍 없이 완전히 처리된 후 한번에 돌려주면, 사용자 입장에선 **“버튼 누르고 한참 있다 결과 받는”** 형태가 되어 답답할 수 있다. 현재 많은 LLM 애플리케이션들이 스트리밍으로 응답 속도를 체감상 높이고 있으므로, 우리 도구도 이를 지원하는 것이 바람직하다.
- **병목 및 최적화 지점:**
    - *LLM 토큰 한계:* 코드가 아주 방대하다면 LLM 프롬프트의 토큰 수 한계를 넘을 수 있다. 이 경우 코드를 분할하여 여러 차례 호출해야 하는데, 이는 응답 시간을 늘리고 복원 로직을 복잡하게 만든다. 따라서 코드 길이에 따른 **샤딩 전략**이나 요약(few-shot) 등의 기법을 고려해야 한다. 그러나 이는 일반적인 코드 조각 수준에서는 크게 문제되지 않으며, 주로 수천 줄 이상의 대용량일 때의 이슈다.
    - *동시성:* 여러 사용자가 동시에 이 시스템을 사용하면 LLM API의 **동시 요청 부하**나 **요금** 문제가 생길 수 있다. API 호출은 보통 rate limit이 있으므로, MCP 서버에서 큐잉이나 임계치 제어를 해야 한다. 응답속도 자체는 개별 사용자 기준으로는 동일하지만, 과부하 시 지연이 늘어날 수 있다. 이를 해결하기 위해 요청 캐시(동일 코드 반복 분석 시 결과 재사용)나, 또는 일부 작업에 대해선 사전준비된 결과 활용 등의 최적화를 생각해볼 수 있다. 하지만 코드 리팩토링 같은 기능은 맥락의존적이라 캐시효과가 크진 않을 것이다.
    - *MCP 툴 체인 오버헤드:* MCP를 활용하면 AI 에이전트가 자체적으로 API 호출 등을 할 때보다 **효율적으로 작업**할 수 있다는 보고가 있다. Twilio의 테스트에 따르면, MCP로 툴을 사용한 경우 에이전트 작업이 평균 20% 가량 더 빠르게 완료되고 API 호출도 19% 적게 이루어졌다[twilio.com](https://www.twilio.com/en-us/blog/twilio-alpha-mcp-server-real-world-performance#:~:text=%2A%20Tasks%20completed%20~20.5,times%20it%20hit%20Twilio%E2%80%99s%20APIs). 이는 MCP가 필요한 정보만 정확히 제공하여 모델이 불필요한 시도/실패를 줄였기 때문으로 분석된다. 우리의 경우도 마찬가지로, 마스킹/복원 과정이 자동화되어 사용자나 에이전트가 수동으로 민감정보를 골라내고 편집하는 시간 대비 전체 흐름이 빨라질 수 있다. 즉, **MCP 통합으로 인한 프로토콜 오버헤드보다 얻는 효율 이득이 크다**고 볼 수 있다. 다만 툴 호출이 여러 번 연쇄적으로 일어나면 (예: 코드 분석 후, 다시 리팩토링 툴 호출 등) 매 단계마다 JSON-RPC 통신과 LLM 호출이 반복되어 누적 지연이 증가한다. 이런 **Tool Chain** 형태 사용은 신중히 최적화하거나 한 번의 LLM 프롬프트로 복합 작업을 처리하는 방향으로 개선해야 한다[byteplus.com](https://www.byteplus.com/blog/guide-to-mcp-servers?#:~:text=,evaluating%20MCP%20servers%2C%20consider).
    - *클라이언트-서버 배치:* MCP 서버가 **로컬**에서 동작하면 네트워크 지연이 거의 없지만, 원격 서버로 운영된다면 사용자 단말과의 통신 지연이 추가된다. 가능하면 IDE 플러그인 등과 함께 **로컬 MCP 서버**로 구동하여 LLM API 호출만 외부 통신을 하게 하는 편이 빠르다. 또는 사내망 서버에 배치하여 개발자들이 공동으로 쓰게 할 수도 있는데, 이 경우 내부망 속도가 충분히 빠르면 큰 문제는 없을 것이다.
- **현재 성능 평가 및 전망:** 2025년 현재 MCP 기반 AI 코디(코드비서)들은 상당히 실용적인 속도를 보이고 있다. 여러 기업들이 내부 개발지원 도구에 유사한 프라이버시 레이어를 도입하였고, Salesforce 등에서는 LLM 프롬프트에 대한 **Zero Data Retention**(데이터 미보존)과 **마스킹**으로 안전성을 높이면서도 성능은 실시간 응답에 가깝게 유지하고 있다[salesforce.com](https://www.salesforce.com/artificial-intelligence/trusted-ai/#:~:text=Data%20Masking). 앞서 언급한 Twilio 실험처럼 MCP 도입으로 작업 효율이 좋아진 사례도 있고[twilio.com](https://www.twilio.com/en-us/blog/twilio-alpha-mcp-server-real-world-performance#:~:text=%2A%20Tasks%20completed%20~20.5,times%20it%20hit%20Twilio%E2%80%99s%20APIs), OpenAI도 API 데이터 미사용 정책 등을 펴며 기업들이 속도와 보안을 모두 잡을 수 있게 지원하고 있다[cadosecurity.com](https://www.cadosecurity.com/blog/introducing-masked-ai-an-open-source-library-that-enables-the-usage-of-llm-apis-more-securely#:~:text=Since%20then%2C%20OpenAI%20has%20changed,bug%20leaked%20users%27%20conversation%20histories). 물론 GPT-4 같은 모델 호출은 아직도 수 초의 시간이 들지만, 모델 최적화와 하드웨어 개선으로 지연이 줄고 있고, 경우에 따라 GPT-3.5 등의 속도 빠른 모델을 선택해 트레이드오프하기도 한다. **사용자 경험 측면**에서는, 약간의 지연보다 **결과의 신뢰성**이 더 중요하다는 점을 유념해야 한다. 민감정보가 지켜진다는 안도감과 함께, 결과 코드의 정확도가 높다면 몇 초의 추가 딜레이는 수용될 수 있다. 따라서 속도 튜닝도 중요하지만, 우리 시스템의 최우선 가치는 **“안전한 코드 보조”**에 있음을 기억하고 개발을 진행해야 한다.

**참고자료:** 이 설계 및 평가는 최신 MCP 활용 사례와 데이터 마스킹에 관한 업계 보고서를 바탕으로 했다[cadosecurity.com](https://www.cadosecurity.com/blog/introducing-masked-ai-an-open-source-library-that-enables-the-usage-of-llm-apis-more-securely#:~:text=In%20the%20conclusion%20of%20our,to%20ChatGPT%2C%20Raising%20Security%20Fears)[twilio.com](https://www.twilio.com/en-us/blog/twilio-alpha-mcp-server-real-world-performance#:~:text=%2A%20Tasks%20completed%20~20.5,times%20it%20hit%20Twilio%E2%80%99s%20APIs). 앞으로도 모델과 MCP 기술이 발전함에 따라, 더 나은 성능과 안전성을 동시에 달성할 수 있을 것으로 기대된다.

### 백엔드와 프론트엔드 관점에서 개발 플로우

## 1. 개발의 의미

- **표준화된 인터페이스로 보안 강화**
    
    MCP(Model Context Protocol)는 LLM과 외부 도구(API, DB 등)를 연결하는 “공용 어댑터” 역할을 합니다. 이를 활용하면 코드 내 민감정보(키, 내부 변수명 등)를 마스킹한 뒤 안전하게 LLM에 전달하고, 응답 결과를 복원하는 과정을 **일관된 프로토콜**로 처리할 수 있습니다 [stytch](https://stytch.com/blog/model-context-protocol-introduction/?utm_source=chatgpt.com).
    
- **프론트·백 분리로 유지보수 용이**
    
    MCP는 “AI 모델(생각) ↔ 도구(행동)” 사이에 명확한 경계를 만들어 줍니다. 덕분에 프론트엔드와 백엔드가 각자 **자기 책임** 아래 UI와 도구 호출 로직을 독립적으로 발전시킬 수 있어, 협업·확장성·보안 관리 측면에서 큰 이점을 가집니다 [builder](https://www.builder.io/blog/model-context-protocol?utm_source=chatgpt.com).
    
- **재사용 가능한 블록 체계**
    
    마스킹, 코드 분석, LLM 호출, 복원 같은 기능을 각각 MCP 서버(툴)로 제공해 두면, 다른 AI 도구나 워크플로우에서도 “레고 블록처럼” 조합하여 빠르게 새로운 자동화 에이전트를 구축할 수 있습니다 [medium](https://medium.com/intuitionmachine/the-ai-native-revolution-how-model-context-protocol-is-radically-changing-software-1e610b97f6c4?utm_source=chatgpt.com).
    

---

## 2. 백엔드 개발 플로우

1. **프로젝트 초기 설정**
    - Spring Boot 프로젝트 생성
    - MCP 서버용 SDK(Java/Python/TypeScript 중 선택) 설치 또는 JSON‑RPC 직접 구현
    - GitLab 리포지토리 및 CI 파이프라인(Jenkins) 구성
2. **민감정보 전처리 모듈 개발**
    - AST 기반 파서(예: JavaParser) 또는 정규식·시크릿 스캐너(TruffleHog/Gitleaks)로 **마스킹 대상 식별**
    - `<MASK_1>`, `<APIKEY_1>` 등의 토큰으로 **치환**하고, 원본⇄토큰 매핑 테이블(메모리 또는 인메모리 DB) 관리
3. **MCP 서버 툴 정의**
    - `maskCode(code: string) → {maskedCode, mapId}`
    - `analyzeCode(maskedCode, taskType) → maskedResult`
    - `restoreCode(maskedResult, mapId) → finalCode`
        
        위 세 가지 툴을 MCP 프로토콜에 맞춰 JSON‑RPC 메서드로 구현
        
4. **LLM 연동 로직**
    - OpenAI/GPT‑4, Claude API 클라이언트 구성
    - MCP 툴 호출 시, 내부에서 LLM 요청을 생성하고 응답(스트리밍 가능)을 받아오는 서비스 레이어 개발
    - 프롬프트에 “MASK 토큰은 절대 변경 금지” 등의 시스템 지시 포함
5. **후처리 및 검증**
    - LLM 응답에서 토큰 복원
    - 복원된 코드에 대해 **구문 검증**(컴파일/파싱 테스트) 및 **자동 생성된 테스트**(옵션) 실행
    - 오류 발생 시 **롤백** 또는 사용자 알림
6. **배포 및 운영**
    - Dockerize: Spring Boot + MCP 서버 컨테이너화
    - EC2에 배포, Jenkins CI/CD 파이프라인 설정
    - 모니터링: 로그 필터링(민감정보 제거), 메트릭 수집

---

## 3. 프론트엔드 개발 플로우

1. **UI 설계 및 에디터 통합**
    - Next.js + React로 SPA 구성
    - Monaco Editor 또는 CodeMirror 같은 **코드 에디터 컴포넌트** 탑재
2. **MCP 클라이언트 연동**
    - MCP JavaScript/TypeScript SDK 설치
    - 에디터에서 “코드 제출 → maskCode 호출 → analyzeCode 호출 → restoreCode 호출” 순서로 **툴 체인** 구성
    - 요청 상태(진행 중, 스트리밍 중, 완료)를 **실시간 표시**
3. **스트리밍 응답 처리**
    - MCP의 SSE(Server‑Sent Events) 스트리밍으로 LLM 토큰을 **부분 렌더링**
    - 리팩토링 결과를 **Diff 뷰**로 보여주고, 사용자 수락 후 에디터에 반영
4. **사용자 피드백 및 오류 처리**
    - 복원 누락 토큰, 문법 오류 등은 **알림 배너**로 안내
    - 재요청 버튼, 세션 기반 매핑 테이블 ID 관리
5. **보안·UX 최적화**
    - 코드와 매핑 테이블은 **브라우저 메모리**에만 보관, 네트워크에 노출 금지
    - 민감정보를 포함하지 않는 프리뷰 모드 제공(예: 외부 데모 시)

---

이렇게 백엔드에서는 **MCP 서버 툴**을 중심으로 전처리·LLM 호출·후처리 파이프라인을, 프론트엔드에서는 **MCP 클라이언트**와 코드 에디터를 연결해 사용자가 자연스럽게 “코드 개선” 워크플로우를 경험하도록 구현하면 됩니다.

### MSA를 위해 워크 플로우를 6개의 기준으로 나눔

| 개발자 | 마이크로서비스 | 백엔드 주요 책임 | 프론트엔드 주요 책임 |
| --- | --- | --- | --- |
| **Dev A** | **인증·세션 서비스**
(Auth & Session) | - JWT/세션 발급·검증 API (`POST /auth/login`, `GET /auth/me`) 구현<br/>- 사용자·권한 관리 DB 설계·CRUD<br/>- MCP 호출 전 세션 컨텍스트 관리 (mapId 발급) | - 로그인/로그아웃 UI<br/>- 세션 상태에 따른 에디터 접근 제어<br/>- 사용자 프로필 화면 |
| **Dev B** | **마스킹 서비스**
(Masking) | - 코드 정적분석·패턴 매칭 모듈 (AST, Gitleaks 연동)<br/>- `POST /v1/masks` REST API 및 MCP 툴(`maskCode`) 구현<br/>- 매핑 테이블(mapId↔tokens) 인메모리 관리 | - 코드 에디터 → 마스킹 요청 버튼<br/>- 치환 전/후 비교 뷰<br/>- 매핑 세션 관리 UX |
| **Dev C** | **분석·리팩토링 서비스**
(Analysis) | - LLM 연동 모듈 (GPT‑4/Claude API 클라이언트)<br/>- `POST /v1/analyses` REST API 및 MCP 툴(`analyzeCode`) 구현<br/>- 스트리밍 SSE 응답 처리 및 토큰화 | - 마스킹된 코드에 대한 분석 결과 스트리밍 뷰<br/>- 리팩토링 제안 Diff 뷰<br/>- “적용/무시” 액션 UI |
| **Dev D** | **복원·후처리 서비스**
(Restoration) | - 토큰 역치환 로직 (`<MASK_1>` → 원본) 구현<br/>- `POST /v1/restores` REST API 및 MCP 툴(`restoreCode`) 구현<br/>- 복원 후 구문 검증·테스트 트리거 | - 복원된 최종 코드 표시 UI<br/>- 복원 오류 알림·수정 폼<br/>- 테스트 결과 요약 뷰 |
| **Dev E** | **워크플로우 오케스트레이션**
(Orchestration) | - 각 서비스 호출을 순차·병렬 조율하는 Orchestrator<br/>- `POST /v1/workflows` 엔드포인트 및 MCP 툴 정의<br/>- 에러 핸들링·재시도 로직, 로깅 | - 전체 파이프라인 진행 상태 대시보드<br/>- 단계별 진척률 표시<br/>- 재실행/롤백 버튼 |
| **Dev F** | **모니터링·알림·대시보드**
(Monitoring & Dashboard) | - 로그 수집·분석(ELK/Prometheus 연동)<br/>- 장애·성능 알림 API(`POST /alerts`)<br/>- 서비스 상태·사용량 메트릭 API | - 실시간 모니터링 대시보드(차트)<br/>- 알림 설정 UI<br/>- 팀별 사용 통계 페이지 |

---

### 왜 이렇게 나눌까?

1. **수직 슬라이싱(Vertical Slice)**
    
    각 팀이 **백엔드 API, 비즈니스 로직, 데이터베이스, 프론트엔드 UI**를 모두 소유함으로써 빠른 의사결정과 높은 자율성을 보장합니다 [techtarget](https://www.techtarget.com/searchapparchitecture/tip/A-few-fundamental-microservices-team-structure-strategies?utm_source=chatgpt.com).
    
2. **책임의 명확화**
    - Auth 팀은 **보안·인증**에만 집중
    - Mask/Analysis/Restore 팀은 **코드 처리 파이프라인**에만 집중
    - Orchestration 팀은 **서비스 간 흐름**을 관리
    - Monitoring 팀은 **운영 안정성**을 담당
3. **인터페이스 단순화**
    
    서비스 간 통신은 **RESTful API** 또는 MCP JSON‑RPC 툴 호출로 정의하여, 계약(contract)이 명확합니다. 이는 MSA 성공 사례에서도 강조되는 바입니다 [cerbos](https://www.cerbos.dev/blog/team-collaboration-and-code-ownership-microservices?utm_source=chatgpt.com).
    
4. **확장·유지보수 용이**
    
    서비스가 독립 배포되므로 특정 팀의 변경이 다른 팀에 미치는 영향을 최소화할 수 있습니다.