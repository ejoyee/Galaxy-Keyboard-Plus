# 📘 MCP (Model Context Protocol)

> **LLM에게 어떤 문맥(Context)을 줄 것인가를 표준화한 방식**

---

## ✅ MCP가 필요한 이유

- LLM은 모든 정보를 기억하지 않음
- 입력 토큰 수에는 제한이 있음 (ex: GPT-4는 128K context window)
- 효율적으로 정보를 **선택하고 주입**해야 함
- 문서, 대화, 메모리, DB 등 다양한 출처의 문맥 연결 필요 → **표준 프로토콜 필요**

---

## 🔧 MCP의 핵심 역할

1. **문맥 수집**: 다양한 소스(문서, 대화, 메모리 등)에서 정보 수집
2. **문맥 전처리**: 요약하거나 구조화
3. **문맥 선택**: 질문에 필요한 정보만 선별
4. **모델 호출**: context 포함된 프롬프트로 LLM 호출
5. **세션 관리**: 이전 대화 등 기억하여 연결

---

## 🔁 MCP 작동 순서

1. 사용자가 질문
2. 관련 문서/정보 탐색
3. 중요 문맥 추출
4. context 조합
5. LLM 호출
6. 응답 반환

---

## 🧱 MCP의 구성 요소

### 1. 📜 프롬프트 엔지니어링

- 어떤 방식으로 어떤 정보를 전달할지 설계

### 2. 🧠 Token 제한 & Context Window

- 모델에 넣을 수 있는 최대 토큰 수
- 중요 문서만 선별하거나 요약

### 3. 🧩 Embedding + Vector DB

- 문서를 벡터로 변환하여 질문과 유사도 높은 문서 검색
- 대표적 벡터DB: `FAISS`, `Pinecone`, `Weaviate`

### 4. 🔍 RAG (Retrieval-Augmented Generation)

- 외부 검색 + 생성 결합 → LLM이 모르는 정보도 반영 가능

### 5. 🧾 Memory & Session Management

- 사용자 별 기억을 유지 → 대화 맥락, 선호도 반영 가능

---

## 📂 MCP 유형별 패턴

| 유형                         | 설명                                  |
| ---------------------------- | ------------------------------------- |
| **Static Context Protocol**  | 고정 문맥 사용 (프롬프트에 안내문 등) |
| **Dynamic Context Protocol** | 벡터 DB 기반으로 매번 문맥을 구성     |
| **Memory-based Protocol**    | 사용자의 메모리 활용                  |
| **Tool-enhanced Protocol**   | 웹 검색, API 호출 등 외부 툴 사용     |
| **Multi-source Protocol**    | 다양한 소스(Drive, Notion 등) 통합    |

---

## 🖥️ MCP 서버 유형 (구현체 예시)

| 서버 유형                     | 설명                                      |
| ----------------------------- | ----------------------------------------- |
| **Driver MCP Server**         | Google Drive, Notion 등 문서 기반 context |
| **Knowledge Base MCP Server** | 위키, PDF, DB 등 지식 통합                |
| **Personal AI MCP Server**    | 사용자 성향 및 대화 기억                  |
| **Tool-Enabled MCP Server**   | API, 계산기, 웹 검색 등 연결              |
| **Multi-Agent MCP Server**    | 여러 LLM이 역할을 나눠 협업               |

---

## 🔎 RAG (Retrieval-Augmented Generation)

> 검색 기반 + 생성형 AI의 결합

### 🔁 동작 흐름

1. 문서 수집 (웹, PDF, DB 등)
2. 문서 분할 (청크 단위)
3. **Embedding 생성** (문장을 벡터로)
4. Vector DB에 저장
5. 질문 입력
6. 벡터 검색 → 유사 문서 찾기
7. 프롬프트 구성 (문맥 포함)
8. LLM 호출 → 응답 생성

---

## ⚙️ LangChain과 MCP

| 요소                   | 설명                                    |
| ---------------------- | --------------------------------------- |
| **LangChain**          | LLM을 앱처럼 활용하게 해주는 프레임워크 |
| **DocumentLoader**     | 다양한 문서 로딩                        |
| **Retriever / Memory** | 문맥 생성, Embedding 및 검색            |
| **LLMChain / Agent**   | 문맥 + 모델 호출 흐름 구성              |

---

## 📌 MCP 서버 내 LangChain과 RAG의 역할

- `LangChain` → **모든 MCP 요소들을 연결하고 조율하는 흐름 담당**
- `RAG` → **문서 기반 정보를 모델에 제공하는 핵심 방법**
