import asyncio
import logging
import time
import json
import re
from typing import List, Dict, Optional
from app.utils.ai_utils import expand_info_query, generate_info_answer, openai_client
from app.utils.semantic_search import search_similar_items_enhanced_optimized
from app.utils.context_helpers import check_if_requires_context, get_chat_context, generate_contextualized_info_answer
from app.config.settings import MAX_CONTEXT_ITEMS
from concurrent.futures import ThreadPoolExecutor

logger = logging.getLogger(__name__)
executor = ThreadPoolExecutor(max_workers=10)


async def process_info_search(user_id: str, query: str, timings: Dict) -> Dict:
    """ì •ë³´ ê²€ìƒ‰ ì²˜ë¦¬ - ì´ì „ ëŒ€í™” ê¸°ë¡ í™œìš©"""
    # ë§¥ë½ í•„ìš” ì—¬ë¶€ í™•ì¸
    requires_context = check_if_requires_context(query)
    chat_history = []
    
    if requires_context:
        # ì´ì „ ëŒ€í™” ê¸°ë¡ ê°€ì ¸ì˜¤ê¸°
        chat_history, context_timings = await get_chat_context(user_id, query)
        timings.update(context_timings)
        logger.info(f"ğŸ” ì´ì „ ëŒ€í™” ê²€ìƒ‰: {len(chat_history)}ê°œ ({timings.get('context_retrieval', 0):.3f}ì´ˆ)")
    query_expand_start = time.time()
    expanded_queries = await expand_info_query(query)
    timings["query_expansion"] = time.time() - query_expand_start
    logger.info(f"ğŸ” í™•ì¥ëœ ì¿¼ë¦¬: {expanded_queries}")

    # ë²¡í„° ê²€ìƒ‰ìœ¼ë¡œ ê´€ë ¨ ì •ë³´ ì°¾ê¸°
    vector_search_start = time.time()
    namespace = f"{user_id}_information"
    loop = asyncio.get_event_loop()

    logger.info(
        f"ğŸ” ë²¡í„° ê²€ìƒ‰ ì‹œì‘ - namespace: {namespace}, queries: {expanded_queries}"
    )

    # ê²€ìƒ‰ ë¡œì§
    context_info = await perform_vector_search(user_id, expanded_queries, query)

    timings["vector_search"] = time.time() - vector_search_start
    logger.info(
        f"ğŸ“š ê²€ìƒ‰ëœ ì •ë³´: {len(context_info)}ê°œ ({timings['vector_search']:.3f}ì´ˆ)"
    )

    # ë‹µë³€ ìƒì„±
    answer_start = time.time()
    
    if chat_history:
        # ì •ë³´ì™€ ë§¥ë½ì„ ëª¨ë‘ ê³ ë ¤í•œ ë‹µë³€ ìƒì„±
        answer, used_info_indices, used_chat = await generate_contextualized_info_answer(
            user_id, query, context_info, chat_history
        )
        logger.info(f"âœï¸ ë§¥ë½ ê¸°ë°˜ ë‹µë³€ ìƒì„± (ì‚¬ìš©ëœ ì •ë³´: {len(used_info_indices)}ê°œ, ì‚¬ìš©ëœ ëŒ€í™”: {len(used_chat)}ê°œ)")
    else:
        # ì •ë³´ë§Œ ê³ ë ¤í•œ ë‹µë³€ ìƒì„±
        answer, used_info_indices = await generate_enhanced_info_answer(user_id, query, context_info)
        used_chat = []
        logger.info(f"âœï¸ ì¼ë°˜ ë‹µë³€ ìƒì„± (ì‚¬ìš©ëœ ì •ë³´: {len(used_info_indices)}ê°œ)")

    timings["answer_generation"] = time.time() - answer_start
    logger.info(f"âœï¸ ë‹µë³€ ìƒì„± ì™„ë£Œ ({timings['answer_generation']:.3f}ì´ˆ)")
    
    # ì‘ë‹µì— ì‹¤ì œë¡œ ì‚¬ìš©ëœ ì •ë³´ ì†ŒìŠ¤ì˜ IDë§Œ ì¶”ì¶œ
    photo_ids = []
    if used_info_indices and context_info:
        for idx in used_info_indices:
            if idx < len(context_info):
                item_id = extract_id_from_item(context_info[idx])
                if item_id:
                    photo_ids.append(item_id)
    
    logger.info(f"ğŸ†” ë‹µë³€ì— ì‚¬ìš©ëœ ID: {photo_ids}")

    # ê²°ê³¼ êµ¬ì„±
    return {
        "type": "info_search",
        "query": query,
        "answer": answer,
        "context_count": len(context_info),
        "chat_context_used": len(used_chat) > 0,  # ëŒ€í™” ë§¥ë½ ì‚¬ìš© ì—¬ë¶€
        "photo_ids": photo_ids,
        "_timings": timings,
        "_debug": {
            "expanded_queries": expanded_queries,
            "namespace": namespace,
            "context_sample": (
                context_info[0].get("text", "")[:200] if context_info else None
            ),
            "chat_history_count": len(chat_history),
        },
    }


async def perform_vector_search(
    user_id: str, expanded_queries: List[str], original_query: str
) -> List[Dict]:
    """ë²¡í„° ê²€ìƒ‰ ìˆ˜í–‰"""
    context_info = []
    loop = asyncio.get_event_loop()

    try:
        # 1. í™•ì¥ëœ ì¿¼ë¦¬ë¡œ ê²€ìƒ‰
        result1 = await loop.run_in_executor(
            executor,
            search_similar_items_enhanced_optimized,
            user_id,
            expanded_queries,
            "information",
            20,
        )
        context_info.extend(result1)
        logger.info(f"âœ… í™•ì¥ëœ ì¿¼ë¦¬ ê²°ê³¼: {len(result1)}ê°œ")

        # 2. ì›ë³¸ ì¿¼ë¦¬ë¡œë„ ê²€ìƒ‰
        if len(context_info) < 5:
            result2 = await loop.run_in_executor(
                executor,
                search_similar_items_enhanced_optimized,
                user_id,
                [original_query],
                "information",
                10,
            )
            context_info.extend(result2)
            logger.info(f"âœ… ì›ë³¸ ì¿¼ë¦¬ ê²°ê³¼: {len(result2)}ê°œ")

        # ì¤‘ë³µ ì œê±°
        seen_texts = set()
        unique_results = []
        for item in context_info:
            text = item.get("text", "")
            if text and text not in seen_texts:
                seen_texts.add(text)
                unique_results.append(item)

        return unique_results[:MAX_CONTEXT_ITEMS]

    except Exception as e:
        logger.error(f"âŒ ë²¡í„° ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}", exc_info=True)
        return []


async def generate_enhanced_info_answer(
    user_id: str, query: str, context_info: List[Dict]
) -> tuple[str, List[int]]:
    """ê°œì„ ëœ ì •ë³´ ê¸°ë°˜ ë‹µë³€ ìƒì„± - ì‚¬ìš©ëœ ì»¨í…ìŠ¤íŠ¸ ì¸ë±ìŠ¤ ë°˜í™˜ & ë¶ˆì¶©ë¶„í•œ ì •ë³´ì—ë„ ëŒ€ì‘"""

    def sync_generate_enhanced_answer():
        # context ì •ë³´ë¥¼ ë” ì²´ê³„ì ìœ¼ë¡œ ì •ë¦¬
        context_texts = []
        for i, item in enumerate(context_info[:5]):  # ìƒìœ„ 5ê°œë§Œ ì‚¬ìš©
            text = item.get("text", "").strip()
            if text:
                context_texts.append(f"{i+1}. {text}")

        context_text = "\n".join(context_texts)

        # ì‚¬ìš©ëœ ì»¨í…ìŠ¤íŠ¸ ì¸ë±ìŠ¤ë¥¼ ì¶”ì í•˜ê¸° ìœ„í•œ í”„ë¡¬í”„íŠ¸ ì¶”ê°€
        if context_text:
            prompt = f"""
ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì•„ë˜ ì œê³µëœ ì •ë³´ë¥¼ í™œìš©í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”.
ì •ë³´ê°€ ë¶€ì¡±í•˜ë”ë¼ë„ ìµœëŒ€í•œ ê´€ë ¨ëœ ë‚´ìš©ì„ ì¶”ì¶œí•˜ì—¬ ìì—°ìŠ¤ëŸ¬ìš´ ë‹µë³€ì„ êµ¬ì„±í•˜ì„¸ìš”.

[ì œê³µëœ ì •ë³´]
{context_text}

[ì‚¬ìš©ì ì§ˆë¬¸]
{query}

ë‹µë³€ ì‘ì„± ê·œì¹™:
1. ì œê³µëœ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì •í™•í•˜ê²Œ ë‹µë³€
2. ì¹œê·¼í•˜ê³  ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€í™”ì²´ ì‚¬ìš©
3. ì •ë³´ê°€ ë¶€ì¡±í•œ ê²½ìš°ì—ë„ ì§ˆë¬¸ì— ë§ëŠ” ë‹µë³€ ì œê³µ
4. ì•Œë ¤ì§„ ì •ë³´ë§Œìœ¼ë¡œ ë‹µë³€í•˜ë˜, ì˜ë¯¸ìˆëŠ” ì •ë³´ê°€ ì „í˜€ ì—†ëŠ” ê²½ìš° ì ì ˆíˆ ì•ˆë‚´
5. ë°˜ë“œì‹œ ì‚¬ìš©í•œ ì •ë³´ì˜ ë²ˆí˜¸ë¥¼ ë§ˆì§€ë§‰ì— ëª©ë¡ìœ¼ë¡œ ì¶”ê°€ (1, 3, 5ì²˜ëŸ¼ ìˆ«ìë§Œ ì“°ê³  ê° ìˆ«ìëŠ” ì‰¼í‘œë¡œ êµ¬ë¶„)

ë‹µë³€:
"""
        else:
            # ì»¨í…ìŠ¤íŠ¸ê°€ ì—†ëŠ” ê²½ìš°
            prompt = f"""
ì‚¬ìš©ìì˜ ì§ˆë¬¸: "{query}"

ì§ˆë¬¸ì— ëŒ€í•œ ì •í™•í•œ ì •ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆì§€ë§Œ, ìµœëŒ€í•œ ê´€ë ¨ëœ ë‚´ìš©ì„ ì œê³µí•´ë³´ì„¸ìš”. 
ì •ë³´ê°€ ë¶€ì¡±í•˜ë”ë¼ë„ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ìœ ìš©í•œ ë‹µë³€ì„ ì œê³µí•˜ë˜, ì¶”ì¸¡ì„ì„ ì§€í•˜ê³  ì •í™•í•œ ì •ë³´ë¥¼ ì œê³µí•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.

ë‹µë³€:
"""

        response = openai_client.chat.completions.create(
            model="gpt-4o-mini",  # ë‹¤ë¥¸ ëª¨ë¸ë¡œ ë³€ê²½ ê°€ëŠ¥
            messages=[
                {
                    "role": "system",
                    "content": "ë„ˆëŠ” ì‚¬ìš©ìì˜ ê°œì¸ ë¹„ì„œì•¼. ì œê³µëœ ì •ë³´ë¥¼ í™œìš©í•´ ì •í™•í•˜ê³  ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ í•´ì¤˜. ì–´ë–¤ ì •ë³´ë¥¼ ì‚¬ìš©í–ˆëŠ”ì§€ í‘œì‹œí•˜ë¼.",
                },
                {"role": "user", "content": prompt},
            ],
            temperature=0.7,
        )

        answer = response.choices[0].message.content.strip()

        # ì‚¬ìš©ëœ ì»¨í…ìŠ¤íŠ¸ ì¸ë±ìŠ¤ ì¶”ì¶œ
        used_indices = []
        if context_text:  # ì»¨í…ìŠ¤íŠ¸ê°€ ìˆì—ˆì„ ë•Œë§Œ ì¶”ì¶œ
            # ë‹µë³€ ëë¶€ë¶„ì—ì„œ ë²ˆí˜¸ ëª©ë¡ ì¶”ì¶œ
            indices_pattern = r"\b([0-9]+(?:,\s*[0-9]+)*)\b"
            indices_matches = re.findall(indices_pattern, answer.split("\n")[-1])

            if indices_matches:
                # ë§ˆì§€ë§‰ ë³€ì—ì„œ ë°›ì€ ê²ƒì´ ë¦¬ìŠ¤íŠ¸ì˜ í˜•íƒœë¡œ ë„ì¶œë˜ë©´ ê·¸ê±¸ ì‚¬ìš©
                last_match = indices_matches[-1]
                for idx_str in last_match.split(","):
                    try:
                        idx = int(idx_str.strip()) - 1  # 1-based -> 0-based
                        if 0 <= idx < len(context_info):
                            used_indices.append(idx)
                    except ValueError:
                        continue

            # ìˆ˜ì²˜ë¦¬ëœ ë§ˆì§€ë§‰ í–‰ì„ ì œê±° (ì™¸ë¶€ì—ì„œ ë³´ì´ì§€ ì•Šê²Œ)
            if used_indices and "\n" in answer:
                lines = answer.split("\n")
                if any(
                    all(c in "0123456789, " for c in line.strip())
                    for line in lines[-2:]
                ):
                    answer = "\n".join(lines[:-1]).strip()

        return answer, used_indices

    loop = asyncio.get_event_loop()
    return await loop.run_in_executor(executor, sync_generate_enhanced_answer)


def extract_id_from_item(item: Dict) -> Optional[str]:
    """í•œ ê°œì˜ ê²€ìƒ‰ ê²°ê³¼ í•­ëª©ì—ì„œ ID ì¶”ì¶œ"""
    # IDê°€ ì´ë¯¸ ìˆëŠ” ê²½ìš°
    if "id" in item and item["id"] not in ["unknown", ""]:
        return item["id"]

    text = item.get("text", "")
    if not text:
        return None

    # 1. ë‹¨ìˆœ ìˆ«ìë¡œ ì‹œì‘í•˜ëŠ” ì˜ìˆ˜ì¦ ë²ˆí˜¸ íŒ¨í„´
    receipt_id_match = re.match(r"^(\d{5,10})", text.strip())
    if receipt_id_match:
        return receipt_id_match.group(1)

    # 2. "ìˆ«ì: " í˜•íƒœì˜ ID
    prefix_id_match = re.match(r"^(\d+):\s", text.strip())
    if prefix_id_match:
        return prefix_id_match.group(1)

    # 3. ì²« ì¤„ì´ ìˆ«ìë¡œë§Œ ì´ë£¨ì–´ì§„ ê²½ìš°
    first_line = text.strip().split("\n")[0].strip() if "\n" in text else ""
    if first_line and first_line.isdigit() and len(first_line) > 4:
        return first_line

    return None
