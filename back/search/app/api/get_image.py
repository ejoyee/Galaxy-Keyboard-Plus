from fastapi import APIRouter, Form
from typing import Optional, List, Dict
from app.utils.semantic_search import search_similar_items_enhanced_optimized
from app.utils.chat_vector_store import save_chat_vector_to_pinecone
import json
import time
import asyncio
from concurrent.futures import ThreadPoolExecutor
import logging
import psycopg2
import os
from openai import OpenAI
from dotenv import load_dotenv
import hashlib
from datetime import datetime, timedelta
import traceback

load_dotenv()
router = APIRouter()
logger = logging.getLogger(__name__)

# ThreadPoolExecutor
executor = ThreadPoolExecutor(max_workers=10)

# DB ì—°ê²° ì„¤ì •
DB_PARAMS = {
    "host": "3.38.95.110",
    "port": "5434",
    "database": os.getenv("POSTGRES_RAG_DB_NAME"),
    "user": os.getenv("POSTGRES_RAG_USER"),
    "password": os.getenv("POSTGRES_RAG_PASSWORD"),
}

# OpenAI í´ë¼ì´ì–¸íŠ¸
openai_client = OpenAI(api_key=os.getenv("OPENAI_API_KEY_2"))

# ìºì‹œ ì„¤ì •
cache: Dict[str, Dict] = {}
CACHE_TTL_SECONDS = 3600
MAX_CACHE_SIZE = 500


def get_cache_key(user_id: str, query: str) -> str:
    """ìºì‹œ í‚¤ ìƒì„±"""
    cache_data = f"image:{user_id}:{query}"
    return hashlib.md5(cache_data.encode()).hexdigest()


def get_from_cache(key: str) -> Optional[Dict]:
    """ìºì‹œì—ì„œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°"""
    if key in cache:
        cached_data = cache[key]
        if datetime.now() < cached_data["expires_at"]:
            logger.info(f"âœ… ìºì‹œ íˆíŠ¸: {key}")
            return cached_data["data"]
        else:
            del cache[key]
            logger.info(f"ğŸ—‘ï¸ ë§Œë£Œëœ ìºì‹œ ì‚­ì œ: {key}")
    return None


def set_cache(key: str, data: Dict):
    """ìºì‹œì— ë°ì´í„° ì €ì¥"""
    if len(cache) >= MAX_CACHE_SIZE:
        oldest_key = min(cache.keys(), key=lambda k: cache[k]["created_at"])
        del cache[oldest_key]
        logger.info(f"ğŸ—‘ï¸ ìºì‹œ í¬ê¸° ì´ˆê³¼ë¡œ ì˜¤ë˜ëœ í•­ëª© ì œê±°: {oldest_key}")

    cache[key] = {
        "data": data,
        "created_at": datetime.now(),
        "expires_at": datetime.now() + timedelta(seconds=CACHE_TTL_SECONDS),
    }
    logger.info(f"ğŸ’¾ ìºì‹œ ì €ì¥: {key}")


async def determine_image_query_intent(query: str) -> str:
    """ì´ë¯¸ì§€ ê´€ë ¨ ì§ˆë¬¸ì˜ ì˜ë„ë¥¼ íŒŒì•…"""

    def sync_determine_intent():
        prompt = f"""
ì‚¬ìš©ìì˜ ì§ˆë¬¸ì´ ë‹¤ìŒ ì¤‘ ì–´ëŠ ì˜ë„ì— í•´ë‹¹í•˜ëŠ”ì§€ ë¶„ì„í•˜ì„¸ìš”:
- "find_photo": ì‚¬ìš©ìê°€ ì‚¬ì§„ì„ ì°¾ê³ ì í•¨
- "get_info": ì‚¬ìš©ìê°€ í…ìŠ¤íŠ¸ ì •ë³´ë‚˜ ì„¤ëª…ì„ ì›í•¨

íŒë‹¨ ê¸°ì¤€:
- ì§ˆë¬¸ì— "ì‚¬ì§„", "ì´ë¯¸ì§€", "ì°ì€", "ë³´ì—¬ì¤˜" ë“±ì˜ ë‹¨ì–´ê°€ í¬í•¨ë˜ì–´ ìˆìœ¼ë©´ find_photo
- ì§ˆë¬¸ì´ ë‹¨ì–´ í•˜ë‚˜ë¿ì¸ ê²½ìš°ì—ë„ ê·¸ ë‹¨ì–´ê°€ ì¥ì†Œ, ì¸ë¬¼, ì‚¬ë¬¼ ë“± ì¼ë°˜ì ìœ¼ë¡œ ì‚¬ì§„ì— ë“±ì¥í•  ìˆ˜ ìˆëŠ” í‚¤ì›Œë“œì´ë©´ find_photo
- ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ get_info

ì˜ˆì‹œ:
- "í—¬ë¡œí‚¤í‹°" â†’ find_photo
- "í—¬ë¡œí‚¤í‹° ì‚¬ì§„" â†’ find_photo
- "í—¬ë¡œí‚¤í‹°ëŠ” ëˆ„êµ¬ì•¼?" â†’ get_info

ì§ˆë¬¸: {query}

ì‘ë‹µì€ ë°˜ë“œì‹œ "find_photo" ë˜ëŠ” "get_info" ì¤‘ í•˜ë‚˜ë§Œ ì£¼ì„¸ìš”.
"""

        response = openai_client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {
                    "role": "system",
                    "content": "ë„ˆëŠ” ì˜ë„ ë¶„ë¥˜ ì „ë¬¸ê°€ì•¼. ì •í™•í•˜ê²Œ ë¶„ë¥˜í•´ì¤˜.",
                },
                {"role": "user", "content": prompt},
            ],
            temperature=0,
        )

        return response.choices[0].message.content.strip().lower()

    loop = asyncio.get_event_loop()
    return await loop.run_in_executor(executor, sync_determine_intent)


async def extract_photo_keywords(query: str) -> List[str]:
    """ì‚¬ì§„ ê²€ìƒ‰ì„ ìœ„í•œ í‚¤ì›Œë“œ ì¶”ì¶œ - ì§ì ‘ ê´€ë ¨ í‚¤ì›Œë“œì™€ ë‚ ì§œ í‘œí˜„ ì²˜ë¦¬"""

    def sync_extract_keywords():
        # 1. í‚¤ì›Œë“œ ì¶”ì¶œ í”„ë¡¬í”„íŠ¸
        prompt = f"""
ë‹¤ìŒ ì§ˆë¬¸ì—ì„œ ì‚¬ì§„ì„ ì°¾ê¸° ìœ„í•œ ì§ì ‘ ê´€ë ¨ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ì„¸ìš”.
ì§ˆë¬¸ê³¼ ì§ì ‘ ê´€ë ¨ëœ í•µì‹¬ ë‹¨ì–´ì™€ ìœ ì‚¬ì–´ë§Œ ì¶”ì¶œí•˜ê³ , ê´€ë ¨ì„±ì´ ë‚®ì€ í™•ì¥ í‚¤ì›Œë“œëŠ” ì œì™¸í•©ë‹ˆë‹¤.
ë‚ ì§œ/ì‹œê°„ í‘œí˜„ì€ ì œì™¸í•˜ê³  ë‹¤ë¥¸ í‚¤ì›Œë“œë§Œ ì¶”ì¶œí•˜ì„¸ìš”.

ì§ˆë¬¸: {query}

ì¶”ì¶œ ê·œì¹™:
1. ì§ˆë¬¸ê³¼ ì§ì ‘ ê´€ë ¨ëœ ì£¼ìš” ëª…ì‚¬ì™€ í˜•ìš©ì‚¬ë§Œ ì¶”ì¶œ
2. í•µì‹¬ ê°œë…ì˜ ìœ ì‚¬ì–´ì™€ ê´€ë ¨ì–´ í¬í•¨ (ìµœëŒ€ 2-3ê°œ)
3. ê°„ì ‘ì ìœ¼ë¡œ ì—°ê´€ëœ í™•ì¥ í‚¤ì›Œë“œëŠ” ì œì™¸
4. ë™ì‚¬ëŠ” ê´€ë ¨ ëª…ì‚¬ë¡œë§Œ ë³€í™˜ (ì˜ˆ: "ë¨¹ë‹¤" â†’ "ì‹ì‚¬")
5. ë‚ ì§œ/ì‹œê°„ í‘œí˜„ì€ ë³„ë„ë¡œ ì²˜ë¦¬ë˜ë¯€ë¡œ ì œì™¸

ì˜ˆì‹œ:
"ìƒì¼ íŒŒí‹° ì‚¬ì§„" â†’ ["ìƒì¼", "íŒŒí‹°", "ì¶•í•˜"]
"ì–´ì œ íšŒì‚¬ì—ì„œ ì°ì€ ì‚¬ì§„" â†’ ["íšŒì‚¬", "ì‚¬ë¬´ì‹¤", "ì§ì¥"]
"ì§€ë‚œì£¼ í•´ë³€ì—ì„œ ì°ì€ ì‚¬ì§„" â†’ ["í•´ë³€", "ë°”ë‹¤", "ë°”ë‹·ê°€"]

JSON ë°°ì—´ë¡œë§Œ ë°˜í™˜í•˜ì„¸ìš”.
"""

        keyword_response = openai_client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {
                    "role": "system",
                    "content": "í‚¤ì›Œë“œ ì¶”ì¶œ ì „ë¬¸ê°€. ì‚¬ì§„ ê²€ìƒ‰ì— ì§ì ‘ ê´€ë ¨ëœ í•µì‹¬ í‚¤ì›Œë“œë§Œ ì¶”ì¶œ. JSON ë°°ì—´ë§Œ ë°˜í™˜.",
                },
                {"role": "user", "content": prompt},
            ],
            temperature=0.2,
        )

        keywords_raw = keyword_response.choices[0].message.content

        # ì½”ë“œë¸”ë¡ ì œê±°
        if "```" in keywords_raw:
            keywords_raw = (
                keywords_raw.replace("```json", "").replace("```", "").strip()
            )

        try:
            keywords = json.loads(keywords_raw)
        except json.JSONDecodeError:
            # JSON íŒŒì‹± ì‹¤íŒ¨ì‹œ ê°„ë‹¨í•œ ì²˜ë¦¬
            keywords = [
                kw.strip() for kw in keywords_raw.strip("[]").split(",") if kw.strip()
            ]

        # 2. ì‹œê°„ í‘œí˜„ì´ ìˆëŠ”ì§€ í™•ì¸í•˜ê³  ë‚ ì§œ ì¶”ì¶œ
        time_words = [
            "ì–´ì œ",
            "ì˜¤ëŠ˜",
            "ë‚´ì¼",
            "ê·¸ì €ê»˜",
            "ëª¨ë ˆ",
            "ì§€ë‚œì£¼",
            "ì´ë²ˆì£¼",
            "ë‹¤ìŒì£¼",
            "ì§€ë‚œë‹¬",
            "ì´ë²ˆë‹¬",
            "ë‹¤ìŒë‹¬",
            "ì‘ë…„",
            "ì˜¬í•´",
            "ë‚´ë…„",
            "ì „ë‚ ",
            "ë‹¤ìŒë‚ ",
        ]

        has_time_expression = any(word in query for word in time_words)

        date_keywords = []
        if has_time_expression:
            # í˜„ì¬ ë‚ ì§œ ê°€ì ¸ì˜¤ê¸°
            current_date = datetime.now()

            # ë‚ ì§œ ì¶”ì¶œ í”„ë¡¬í”„íŠ¸
            date_prompt = f"""
ë‹¤ìŒ ì§ˆë¬¸ì˜ ì‹œê°„ í‘œí˜„ì„ ì˜¤ëŠ˜ ë‚ ì§œ({current_date.strftime('%Yë…„ %mì›” %dì¼')})ë¥¼ ê¸°ì¤€ìœ¼ë¡œ 
ì •í™•í•œ ë‚ ì§œ(YYYYë…„ MMì›” DDì¼)ë¡œ ë³€í™˜í•˜ì„¸ìš”.

ì§ˆë¬¸: {query}

ë°˜í™˜ ê·œì¹™:
1. ë‚ ì§œë§Œ ì¶”ì¶œí•˜ì—¬ "YYYYë…„ MMì›” DDì¼" í˜•ì‹ìœ¼ë¡œ ë°˜í™˜
2. ë‚ ì§œ ë²”ìœ„ê°€ ìˆìœ¼ë©´ ì‹œì‘ì¼ê³¼ ì¢…ë£Œì¼ì„ "YYYYë…„ MMì›” DDì¼~YYYYë…„ MMì›” DDì¼" í˜•ì‹ìœ¼ë¡œ ë°˜í™˜
3. ë‚ ì§œ ì •ë³´ê°€ ì—†ìœ¼ë©´ ë¹ˆ ë°°ì—´([])ì„ ë°˜í™˜

ì˜ˆì‹œ:
- "ì–´ì œ ì°ì€ ì‚¬ì§„" â†’ (ì˜¤ëŠ˜ì´ 2025ë…„ 05ì›” 16ì¼ì¼ ê²½ìš°) ["2025ë…„ 05ì›” 15ì¼"]
- "ì§€ë‚œì£¼ ì—¬í–‰" â†’ ["2025ë…„ 05ì›” 05ì¼~2025ë…„ 05ì›” 11ì¼"]
- "ì´ë²ˆë‹¬ ì´ˆì— ì°ì€ ì‚¬ì§„" â†’ ["2025ë…„ 05ì›” 01ì¼~2025ë…„ 05ì›” 05ì¼"]
- "ì‘ë…„ í¬ë¦¬ìŠ¤ë§ˆìŠ¤" â†’ ["2024ë…„ 12ì›” 25ì¼"]

JSON ë°°ì—´ë¡œë§Œ ë°˜í™˜í•˜ì„¸ìš”.
"""

            date_response = openai_client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {
                        "role": "system",
                        "content": "ë‚ ì§œ ì¶”ì¶œ ì „ë¬¸ê°€. ì§ˆë¬¸ì—ì„œ ì–¸ê¸‰ëœ ì‹œê°„ í‘œí˜„ì„ ì •í™•í•œ ë‚ ì§œë¡œ ë³€í™˜. JSON ë°°ì—´ë§Œ ë°˜í™˜.",
                    },
                    {"role": "user", "content": date_prompt},
                ],
                temperature=0.1,
            )

            date_raw = date_response.choices[0].message.content

            # ì½”ë“œë¸”ë¡ ì œê±°
            if "```" in date_raw:
                date_raw = date_raw.replace("```json", "").replace("```", "").strip()

            try:
                date_data = json.loads(date_raw)
                date_keywords.extend(date_data)
            except json.JSONDecodeError:
                # íŒŒì‹± ì‹¤íŒ¨ì‹œ ë‚ ì§œì— ëŒ€í•œ ê°„ë‹¨í•œ ì²˜ë¦¬
                if "ì–´ì œ" in query:
                    yesterday = current_date - timedelta(days=1)
                    date_keywords.append(yesterday.strftime("%Yë…„ %mì›” %dì¼"))
                elif "ì˜¤ëŠ˜" in query:
                    date_keywords.append(current_date.strftime("%Yë…„ %mì›” %dì¼"))
                elif "ë‚´ì¼" in query:
                    tomorrow = current_date + timedelta(days=1)
                    date_keywords.append(tomorrow.strftime("%Yë…„ %mì›” %dì¼"))

        # 3. í‚¤ì›Œë“œì™€ ë‚ ì§œ í•©ì¹˜ê¸°
        final_keywords = keywords + date_keywords

        # ì¤‘ë³µ ì œê±°
        return list(set(final_keywords))

    loop = asyncio.get_event_loop()
    return await loop.run_in_executor(executor, sync_extract_keywords)


async def search_photos_by_keywords(user_id: str, keywords: List[str]) -> List[Dict]:
    """í‚¤ì›Œë“œë¡œ DBì—ì„œ ì‚¬ì§„ ê²€ìƒ‰ - ë§¤ì¹˜ ì ìˆ˜ í¬í•¨"""

    def sync_search_photos():
        connection = psycopg2.connect(**DB_PARAMS)
        cursor = connection.cursor()

        try:
            # ê°œì„ ëœ ê²€ìƒ‰ ì¿¼ë¦¬ - ë§¤ì¹˜ ì ìˆ˜ì™€ ì •ë³´ í¬í•¨
            query = """
            WITH keyword_matches AS (
                SELECT 
                    i.access_id,
                    i.image_time,
                    i.caption,
                    ik.keyword,
                    CASE 
                        WHEN ik.keyword = ANY(%s) THEN 1.0
                        ELSE 0.5
                    END as match_score
                FROM images i
                JOIN image_keywords ik ON i.access_id = ik.image_id
                WHERE i.user_id = %s 
                AND (
                    ik.keyword = ANY(%s)
                    OR EXISTS (
                        SELECT 1 FROM unnest(%s::text[]) AS search_kw
                        WHERE ik.keyword ILIKE '%%' || search_kw || '%%'
                    )
                )
            ),
            aggregated AS (
                SELECT 
                    access_id,
                    image_time,
                    caption,
                    COUNT(DISTINCT keyword) as keyword_count,
                    SUM(match_score) as total_score,
                    STRING_AGG(DISTINCT keyword, ', ') as matched_keywords
                FROM keyword_matches
                GROUP BY access_id, image_time, caption
            )
            SELECT 
                access_id,
                caption,
                keyword_count,
                total_score,
                matched_keywords,
                image_time
            FROM aggregated
            ORDER BY total_score DESC, keyword_count DESC, image_time DESC
            LIMIT 30;
            """

            cursor.execute(query, (keywords, user_id, keywords, keywords))
            results = cursor.fetchall()

            # ê²°ê³¼ë¥¼ ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
            photo_results = []
            for row in results[:10]:  # ìƒìœ„ 10ê°œë§Œ
                photo_results.append(
                    {
                        "access_id": row[0],
                        "caption": row[1],
                        "match_count": row[2],
                        "score": float(row[3]),
                        "matched_keywords": row[4],
                        "image_time": row[5].isoformat() if row[5] else None,
                    }
                )

                logger.info(
                    f"ğŸ“· ì‚¬ì§„: {row[0]}, ì ìˆ˜: {row[3]:.1f}, ë§¤ì¹˜: {row[2]}ê°œ, í‚¤ì›Œë“œ: {row[4]}"
                )

            return photo_results

        finally:
            cursor.close()
            connection.close()

    loop = asyncio.get_event_loop()
    return await loop.run_in_executor(executor, sync_search_photos)


async def expand_info_query(query: str) -> List[str]:
    """ì •ë³´ ê²€ìƒ‰ì„ ìœ„í•œ ì¿¼ë¦¬ í™•ì¥"""

    def sync_expand_query():
        prompt = f"""
ë‹¤ìŒ ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë‹¤ì–‘í•œ ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ ìƒì„±í•˜ì„¸ìš”.
ì›ë³¸ ì§ˆë¬¸ì˜ ì˜ë¯¸ë¥¼ ìœ ì§€í•˜ë©´ì„œ ë™ì˜ì–´, ê´€ë ¨ì–´, ë‹¤ì–‘í•œ í‘œí˜„ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.

ì›ë³¸ ì§ˆë¬¸: {query}

ë³€í˜• ê·œì¹™:
1. í•µì‹¬ í‚¤ì›Œë“œ í¬í•¨
2. ë™ì˜ì–´ ì‚¬ìš©
3. ì•½ì–´ì™€ ì „ì²´ í‘œí˜„
4. í•œêµ­ì–´ì™€ ì˜ì–´ í˜¼ìš©
5. ê´€ë ¨ ìš©ì–´ ì¶”ê°€

ì˜ˆì‹œ:
"ìŠ¤íƒ€ë²…ìŠ¤ ì™€ì´íŒŒì´ ì•Œë ¤ì¤˜" â†’ 
["ìŠ¤íƒ€ë²…ìŠ¤ WiFi", "ìŠ¤íƒ€ë²…ìŠ¤ ì™€ì´íŒŒì´", "starbucks wifi password", "ìŠ¤íƒ€ë²…ìŠ¤ ì¸í„°ë„·", "ìŠ¤íƒ€ë²…ìŠ¤ ë¬´ì„ ì¸í„°ë„·", "ìŠ¤íƒ€ë²…ìŠ¤ ë¹„ë°€ë²ˆí˜¸", "KT WiFi zone", "ìŠ¤íƒ€ë²…ìŠ¤ ë„¤íŠ¸ì›Œí¬"]

JSON ë°°ì—´ë¡œ 5-7ê°œì˜ ë³€í˜• ì¿¼ë¦¬ë¥¼ ë°˜í™˜í•˜ì„¸ìš”.
"""

        response = openai_client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "ì¿¼ë¦¬ í™•ì¥ ì „ë¬¸ê°€. JSON ë°°ì—´ë§Œ ë°˜í™˜."},
                {"role": "user", "content": prompt},
            ],
            temperature=0.3,
        )

        queries_raw = response.choices[0].message.content

        # ì½”ë“œë¸”ë¡ ì œê±°
        if "```" in queries_raw:
            queries_raw = queries_raw.replace("```json", "").replace("```", "").strip()

        try:
            expanded = json.loads(queries_raw)
            expanded.append(query)  # ì›ë³¸ ì¿¼ë¦¬ í¬í•¨
            return list(set(expanded))[:8]
        except:
            return [query]

    loop = asyncio.get_event_loop()
    return await loop.run_in_executor(executor, sync_expand_query)


async def generate_info_answer(
    user_id: str, query: str, context_info: List[Dict]
) -> str:
    """ì •ë³´ ê¸°ë°˜ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ìƒì„±"""

    def sync_generate_answer():
        # context ì •ë³´ë¥¼ ë” ì²´ê³„ì ìœ¼ë¡œ ì •ë¦¬
        context_texts = []
        for i, item in enumerate(context_info[:5]):  # ìƒìœ„ 5ê°œë§Œ ì‚¬ìš©
            text = item.get("text", "").strip()
            if text:
                context_texts.append(f"{i+1}. {text}")

        context_text = "\n".join(context_texts)

        if not context_text:
            return "ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì§ˆë¬¸ì„ í•´ë³´ì„¸ìš”."

        prompt = f"""
ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì•„ë˜ ì œê³µëœ ì •ë³´ë¥¼ í™œìš©í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”.
ì •ë³´ê°€ ë¶€ì¡±í•˜ë‹¤ë©´ ê·¸ ì‚¬ì‹¤ì„ ì–¸ê¸‰í•˜ê³ , ì•Œë ¤ì§„ ë‚´ìš©ë§Œìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”.

[ì œê³µëœ ì •ë³´]
{context_text}

[ì‚¬ìš©ì ì§ˆë¬¸]
{query}

ë‹µë³€ ì‘ì„± ê·œì¹™:
1. ì œê³µëœ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì •í™•í•˜ê²Œ ë‹µë³€
2. ì¹œê·¼í•˜ê³  ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€í™”ì²´ ì‚¬ìš©
3. ë¶ˆí™•ì‹¤í•œ ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ê³  ëª…ì‹œ
4. í•„ìš”ì‹œ ì¶”ê°€ ì •ë³´ë¥¼ ìš”ì²­í•˜ëŠ” ì•ˆë‚´ í¬í•¨

ë‹µë³€:
"""

        response = openai_client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {
                    "role": "system",
                    "content": "ë„ˆëŠ” ì‚¬ìš©ìì˜ ê°œì¸ ë¹„ì„œì•¼. ì œê³µëœ ì •ë³´ë¥¼ í™œìš©í•´ ì •í™•í•˜ê³  ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ í•´ì¤˜.",
                },
                {"role": "user", "content": prompt},
            ],
            temperature=0.7,
        )

        return response.choices[0].message.content

    loop = asyncio.get_event_loop()
    return await loop.run_in_executor(executor, sync_generate_answer)


async def save_query_async(user_id: str, role: str, content: str, timestamp: int):
    """ì¿¼ë¦¬ ì €ì¥ì„ ìœ„í•œ ë¹„ë™ê¸° ë˜í¼"""
    try:
        loop = asyncio.get_event_loop()
        await loop.run_in_executor(
            executor, save_chat_vector_to_pinecone, user_id, role, content, timestamp
        )
        logger.info(f"âœ… ì¿¼ë¦¬ ì €ì¥ ì™„ë£Œ: {user_id}")
    except Exception as e:
        logger.error(f"âŒ ì¿¼ë¦¬ ì €ì¥ ì‹¤íŒ¨: {user_id} - {str(e)}")


async def save_result_async(user_id: str, role: str, content: str, timestamp: int):
    """ê²°ê³¼ ì €ì¥ì„ ìœ„í•œ ë¹„ë™ê¸° ë˜í¼"""
    try:
        loop = asyncio.get_event_loop()
        await loop.run_in_executor(
            executor, save_chat_vector_to_pinecone, user_id, role, content, timestamp
        )
        logger.info(f"âœ… ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {user_id}")
    except Exception as e:
        logger.error(f"âŒ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {user_id} - {str(e)}")


@router.post("/image/")
async def process_image_query(user_id: str = Form(...), query: str = Form(...)):
    """ì´ë¯¸ì§€ ê´€ë ¨ ì§ˆë¬¸ ì²˜ë¦¬ API"""
    total_start = time.time()

    logger.info(f"ğŸ” ì´ë¯¸ì§€ ì¿¼ë¦¬ ì‹œì‘ - user: {user_id}, query: {query}")

    # ìºì‹œ í™•ì¸
    cache_key = get_cache_key(user_id, query)
    cached_result = get_from_cache(cache_key)
    if cached_result:
        cached_result["_timings"]["total"] = time.time() - total_start
        cached_result["_from_cache"] = True
        return cached_result

    timestamp = int(time.time())
    timings = {}

    try:
        # 1. ì¿¼ë¦¬ ì €ì¥ (ë¹„ë™ê¸°)
        asyncio.create_task(save_query_async(user_id, "user", query, timestamp))

        # 2. ì˜ë„ íŒŒì•…
        intent_start = time.time()
        intent = await determine_image_query_intent(query)
        timings["intent_detection"] = time.time() - intent_start
        logger.info(f"ğŸ¯ ì˜ë„ íŒŒì•…: {intent} ({timings['intent_detection']:.3f}ì´ˆ)")

        if intent == "find_photo":
            # ì‚¬ì§„ ì°¾ê¸° ë¡œì§

            # 3-1. í‚¤ì›Œë“œ ì¶”ì¶œ
            keyword_start = time.time()
            keywords = await extract_photo_keywords(query)
            timings["keyword_extraction"] = time.time() - keyword_start
            logger.info(
                f"ğŸ” ì¶”ì¶œëœ í‚¤ì›Œë“œ ({len(keywords)}ê°œ): {keywords[:10]} ({timings['keyword_extraction']:.3f}ì´ˆ)"
            )

            # 4-1. DBì—ì„œ ì‚¬ì§„ ê²€ìƒ‰
            db_search_start = time.time()
            photo_results = await search_photos_by_keywords(user_id, keywords)
            timings["db_search"] = time.time() - db_search_start

            # photo_idsë§Œ ì¶”ì¶œ
            photo_ids = [photo["access_id"] for photo in photo_results]

            logger.info(
                f"ğŸ“· ê²€ìƒ‰ëœ ì‚¬ì§„: {len(photo_ids)}ê°œ ({timings['db_search']:.3f}ì´ˆ)"
            )

            # ê²°ê³¼ êµ¬ì„±
            result = {
                "type": "photo_search",
                "query": query,
                "keywords": keywords,
                "photo_ids": photo_ids,
                "photo_details": photo_results[:5],  # ìƒìœ„ 5ê°œ ìƒì„¸ ì •ë³´
                "answer": "",
                "count": len(photo_ids),
                "_timings": timings,
            }

        else:  # get_info
            # ì •ë³´ ì°¾ê¸° ë¡œì§

            # 3-2. ì¿¼ë¦¬ í™•ì¥
            query_expand_start = time.time()
            expanded_queries = await expand_info_query(query)
            timings["query_expansion"] = time.time() - query_expand_start
            logger.info(f"ğŸ” í™•ì¥ëœ ì¿¼ë¦¬: {expanded_queries}")

            # 3-3. ë²¡í„° ê²€ìƒ‰ìœ¼ë¡œ ê´€ë ¨ ì •ë³´ ì°¾ê¸°
            vector_search_start = time.time()

            # namespaceë¥¼ user_id_informationìœ¼ë¡œ ì„¤ì •
            namespace = f"{user_id}_information"

            loop = asyncio.get_event_loop()

            logger.info(
                f"ğŸ” ë²¡í„° ê²€ìƒ‰ ì‹œì‘ - namespace: {namespace}, queries: {expanded_queries}"
            )

            # ë¨¼ì € ì›ë³¸ ì¿¼ë¦¬ë¡œ ê²€ìƒ‰
            context_info = []
            try:
                # 1. í™•ì¥ëœ ì¿¼ë¦¬ë¡œ ê²€ìƒ‰
                result1 = await loop.run_in_executor(
                    executor,
                    search_similar_items_enhanced_optimized,
                    user_id,
                    expanded_queries,
                    "info",
                    20,
                )
                context_info.extend(result1)
                logger.info(f"âœ… í™•ì¥ëœ ì¿¼ë¦¬ ê²°ê³¼: {len(result1)}ê°œ")

                # 2. ì›ë³¸ ì¿¼ë¦¬ë¡œë„ ê²€ìƒ‰
                if len(context_info) < 5:
                    result2 = await loop.run_in_executor(
                        executor,
                        search_similar_items_enhanced_optimized,
                        user_id,
                        [query],
                        "info",
                        10,
                    )
                    context_info.extend(result2)
                    logger.info(f"âœ… ì›ë³¸ ì¿¼ë¦¬ ê²°ê³¼: {len(result2)}ê°œ")

                # ì¤‘ë³µ ì œê±°
                seen_texts = set()
                unique_results = []
                for item in context_info:
                    text = item.get("text", "")
                    if text and text not in seen_texts:
                        seen_texts.add(text)
                        unique_results.append(item)

                context_info = unique_results[:20]

                # ê²€ìƒ‰ ê²°ê³¼ ë””ë²„ê¹…
                if context_info:
                    logger.info(f"ğŸ” ê²€ìƒ‰ ê²°ê³¼ ìƒ˜í”Œ:")
                    for i, info in enumerate(context_info[:3]):
                        logger.info(f"  {i+1}. {info.get('text', '')[:100]}...")
                else:
                    logger.warning(f"âš ï¸ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ - namespace: {namespace}")

            except Exception as e:
                logger.error(f"âŒ ë²¡í„° ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}", exc_info=True)
                context_info = []

            timings["vector_search"] = time.time() - vector_search_start
            logger.info(
                f"ğŸ“š ê²€ìƒ‰ëœ ì •ë³´: {len(context_info)}ê°œ ({timings['vector_search']:.3f}ì´ˆ)"
            )

            # 4-2. ë‹µë³€ ìƒì„±
            answer_start = time.time()
            answer = await generate_info_answer(user_id, query, context_info)
            timings["answer_generation"] = time.time() - answer_start
            logger.info(f"âœï¸ ë‹µë³€ ìƒì„± ì™„ë£Œ ({timings['answer_generation']:.3f}ì´ˆ)")

            # ê²°ê³¼ êµ¬ì„±
            result = {
                "type": "info_search",
                "query": query,
                "answer": answer,
                "context_count": len(context_info),
                "photo_ids": [],
                "_timings": timings,
                "_debug": {
                    "expanded_queries": expanded_queries,
                    "namespace": namespace,
                    "context_sample": (
                        context_info[0].get("text", "")[:200] if context_info else None
                    ),
                },
            }

        # ì „ì²´ ì‹œê°„
        timings["total"] = time.time() - total_start
        result["_timings"] = timings
        result["_from_cache"] = False

        # ìºì‹œì— ì €ì¥
        set_cache(cache_key, result)

        # ê²°ê³¼ ì €ì¥ (ë¹„ë™ê¸°)
        asyncio.create_task(
            save_result_async(
                user_id,
                "assistant",
                json.dumps(result, ensure_ascii=False),
                int(time.time()),
            )
        )

        logger.info(
            f"""
â±ï¸ Image API ì„±ëŠ¥ ìš”ì•½:
- ì˜ë„ íŒŒì•…: {timings['intent_detection']:.3f}ì´ˆ
- {'í‚¤ì›Œë“œ ì¶”ì¶œ' if intent == 'find_photo' else 'ì¿¼ë¦¬ í™•ì¥'}: {timings.get('keyword_extraction', timings.get('query_expansion', 0)):.3f}ì´ˆ
- {'DB ê²€ìƒ‰' if intent == 'find_photo' else 'ë²¡í„° ê²€ìƒ‰'}: {timings.get('db_search', timings.get('vector_search', 0)):.3f}ì´ˆ
- {'DB ê²€ìƒ‰' if intent == 'find_photo' else 'ë‹µë³€ ìƒì„±'}: {timings.get('db_search', timings.get('answer_generation', 0)):.3f}ì´ˆ
- ì „ì²´ ì‹œê°„: {timings['total']:.3f}ì´ˆ
        """
        )

        return result

    except Exception as e:
        error_time = time.time() - total_start
        logger.error(
            f"âŒ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ ({error_time:.3f}ì´ˆ): {str(e)}", exc_info=True
        )
        return {
            "error": "ìš”ì²­ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
            "detail": str(e),
            "timings": timings,
            "processing_time": f"{error_time:.3f}ì´ˆ",
        }


# ìºì‹œ ê´€ë¦¬ ì—”ë“œí¬ì¸íŠ¸
@router.get("/image/cache/status")
async def get_cache_status():
    """ìºì‹œ ìƒíƒœ í™•ì¸"""
    current_time = datetime.now()
    valid_count = sum(1 for data in cache.values() if current_time < data["expires_at"])
    expired_count = len(cache) - valid_count

    return {
        "total_items": len(cache),
        "valid_items": valid_count,
        "expired_items": expired_count,
        "max_size": MAX_CACHE_SIZE,
        "ttl_seconds": CACHE_TTL_SECONDS,
    }


@router.delete("/image/cache/clear")
async def clear_cache():
    """ìºì‹œ ì´ˆê¸°í™”"""
    cache_size = len(cache)
    cache.clear()
    logger.info(f"ğŸ—‘ï¸ ìºì‹œ ì´ˆê¸°í™”: {cache_size}ê°œ í•­ëª© ì‚­ì œ")
    return {"cleared_items": cache_size}


# ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•œ ë””ë²„ê·¸ ì—”ë“œí¬ì¸íŠ¸
@router.post("/image/debug-search")
async def debug_search(
    user_id: str = Form(...),
    query: str = Form(...),
    namespace: Optional[str] = Form(None),
):
    """ê²€ìƒ‰ ë””ë²„ê¹…ì„ ìœ„í•œ ì—”ë“œí¬ì¸íŠ¸"""
    try:
        # namespaceê°€ ì œê³µë˜ì§€ ì•Šìœ¼ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©
        if namespace is None:
            namespace = f"{user_id}_information"

        logger.info(
            f"ğŸ” ë””ë²„ê·¸ ê²€ìƒ‰ - user: {user_id}, query: {query}, namespace: {namespace}"
        )

        # ì¿¼ë¦¬ í™•ì¥
        expanded_queries = await expand_info_query(query)
        logger.info(f"ğŸ” í™•ì¥ëœ ì¿¼ë¦¬: {expanded_queries}")

        loop = asyncio.get_event_loop()

        # ì—¬ëŸ¬ ë°©ë²•ìœ¼ë¡œ ê²€ìƒ‰ ì‹œë„
        all_results = []

        # 1. í™•ì¥ëœ ì¿¼ë¦¬ë¡œ ê²€ìƒ‰
        logger.info(f"ğŸ¯ í™•ì¥ëœ ì¿¼ë¦¬ë¡œ ê²€ìƒ‰ ì‹œë„...")
        result1 = await loop.run_in_executor(
            executor,
            search_similar_items_enhanced_optimized,
            user_id,
            expanded_queries,
            "info",
            20,
        )
        all_results.extend(result1)
        logger.info(f"âœ… í™•ì¥ ì¿¼ë¦¬ ê²°ê³¼: {len(result1)}ê°œ")

        # 2. ì›ë³¸ ì¿¼ë¦¬ë¡œë§Œ ê²€ìƒ‰
        logger.info(f"ğŸ¯ ì›ë³¸ ì¿¼ë¦¬ë¡œ ê²€ìƒ‰ ì‹œë„...")
        result2 = await loop.run_in_executor(
            executor,
            search_similar_items_enhanced_optimized,
            user_id,
            [query],
            "info",
            20,
        )
        all_results.extend(result2)
        logger.info(f"âœ… ì›ë³¸ ì¿¼ë¦¬ ê²°ê³¼: {len(result2)}ê°œ")

        # 3. ë‹¨ìˆœ í‚¤ì›Œë“œë¡œ ê²€ìƒ‰
        keywords = query.split()
        logger.info(f"ğŸ¯ í‚¤ì›Œë“œë¡œ ê²€ìƒ‰ ì‹œë„: {keywords}")
        result3 = await loop.run_in_executor(
            executor,
            search_similar_items_enhanced_optimized,
            user_id,
            keywords,
            "info",
            20,
        )
        all_results.extend(result3)
        logger.info(f"âœ… í‚¤ì›Œë“œ ê²°ê³¼: {len(result3)}ê°œ")

        # ì¤‘ë³µ ì œê±°
        unique_results = []
        seen_texts = set()
        for result in all_results:
            text = result.get("text", "")
            if text and text not in seen_texts:
                seen_texts.add(text)
                unique_results.append(result)

        return {
            "query": query,
            "expanded_queries": expanded_queries,
            "namespace": namespace,
            "results_count": len(unique_results),
            "results_breakdown": {
                "expanded_queries_count": len(result1),
                "original_query_count": len(result2),
                "keywords_count": len(result3),
            },
            "results": [
                {
                    "text": result.get("text", "")[:200] + "...",
                    "score": result.get("score", 0),
                    "metadata": result.get("metadata", {}),
                }
                for result in unique_results[:5]
            ],
            "debug_info": {
                "user_id": user_id,
                "namespace_used": namespace,
                "keywords_tried": keywords,
            },
        }
    except Exception as e:
        logger.error(f"âŒ ë””ë²„ê·¸ ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}", exc_info=True)
        return {
            "error": str(e),
            "query": query,
            "namespace": namespace,
            "traceback": traceback.format_exc(),
        }
