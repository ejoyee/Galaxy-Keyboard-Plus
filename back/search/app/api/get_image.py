from fastapi import APIRouter, Form
from typing import Optional, List, Dict
from app.utils.semantic_search import search_similar_items_enhanced_optimized
from app.utils.chat_vector_store import save_chat_vector_to_pinecone
import json
import time
import asyncio
from concurrent.futures import ThreadPoolExecutor
import logging
import psycopg2
import os
from openai import OpenAI
from dotenv import load_dotenv
import hashlib
from datetime import datetime, timedelta
import traceback

load_dotenv()
router = APIRouter()
logger = logging.getLogger(__name__)

# ThreadPoolExecutor
executor = ThreadPoolExecutor(max_workers=10)

# DB ì—°ê²° ì„¤ì •
DB_PARAMS = {
    "host": "3.38.95.110",
    "port": "5434",
    "database": os.getenv("POSTGRES_RAG_DB_NAME"),
    "user": os.getenv("POSTGRES_RAG_USER"),
    "password": os.getenv("POSTGRES_RAG_PASSWORD"),
}

# OpenAI í´ë¼ì´ì–¸íŠ¸
openai_client = OpenAI(api_key=os.getenv("OPENAI_API_KEY_2"))

# ìºì‹œ ì„¤ì •
cache: Dict[str, Dict] = {}
CACHE_TTL_SECONDS = 3600
MAX_CACHE_SIZE = 500


def get_cache_key(user_id: str, query: str) -> str:
    """ìºì‹œ í‚¤ ìƒì„±"""
    cache_data = f"image:{user_id}:{query}"
    return hashlib.md5(cache_data.encode()).hexdigest()


def get_from_cache(key: str) -> Optional[Dict]:
    """ìºì‹œì—ì„œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°"""
    if key in cache:
        cached_data = cache[key]
        if datetime.now() < cached_data["expires_at"]:
            logger.info(f"âœ… ìºì‹œ íˆíŠ¸: {key}")
            return cached_data["data"]
        else:
            del cache[key]
            logger.info(f"ğŸ—‘ï¸ ë§Œë£Œëœ ìºì‹œ ì‚­ì œ: {key}")
    return None


def set_cache(key: str, data: Dict):
    """ìºì‹œì— ë°ì´í„° ì €ì¥"""
    if len(cache) >= MAX_CACHE_SIZE:
        oldest_key = min(cache.keys(), key=lambda k: cache[k]["created_at"])
        del cache[oldest_key]
        logger.info(f"ğŸ—‘ï¸ ìºì‹œ í¬ê¸° ì´ˆê³¼ë¡œ ì˜¤ë˜ëœ í•­ëª© ì œê±°: {oldest_key}")

    cache[key] = {
        "data": data,
        "created_at": datetime.now(),
        "expires_at": datetime.now() + timedelta(seconds=CACHE_TTL_SECONDS),
    }
    logger.info(f"ğŸ’¾ ìºì‹œ ì €ì¥: {key}")


async def determine_image_query_intent(query: str) -> str:
    """ì´ë¯¸ì§€ ê´€ë ¨ ì§ˆë¬¸ì˜ ì˜ë„ë¥¼ íŒŒì•…"""

    def sync_determine_intent():
        prompt = f"""
ë‹¤ìŒ ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ì‚¬ìš©ìì˜ ì˜ë„ë¥¼ íŒŒì•…í•˜ì„¸ìš”.
- íŠ¹ì • ì‚¬ì§„ì„ ì°¾ëŠ” ì§ˆë¬¸ì¸ ê²½ìš°: "find_photo"
- ì •ë³´ë¥¼ ë¬¼ì–´ë³´ëŠ” ì§ˆë¬¸ì¸ ê²½ìš°: "get_info"

ì§ˆë¬¸: {query}

ì‚¬ì§„ì„ ì°¾ëŠ” í‘œí˜„ë“¤:
- "ì‚¬ì§„", "ì´ë¯¸ì§€", "ì°ì€", "ì´¬ì˜í•œ", "ë³´ì—¬ì¤˜", "ì°¾ì•„ì¤˜", "ìˆì–´?", "ì–´ë””"
- íŠ¹ì • ì‹œê°„/ì¥ì†Œ/í™œë™ê³¼ í•¨ê»˜ ì‚¬ì§„ì„ ì–¸ê¸‰í•˜ëŠ” ê²½ìš°

ì •ë³´ë¥¼ ë¬»ëŠ” í‘œí˜„ë“¤:
- "ë‚´ìš©", "ì •ë³´", "ì•Œë ¤ì¤˜", "ì„¤ëª…", "ì–´ë•Œ?", "ë­ì•¼?", "ë¬´ì—‡"
- ì‚¬ì‹¤ì´ë‚˜ ì§€ì‹ì„ ë¬»ëŠ” ê²½ìš°

ì‘ë‹µì€ "find_photo" ë˜ëŠ” "get_info" ì¤‘ í•˜ë‚˜ë§Œ ë°˜í™˜í•˜ì„¸ìš”.
"""

        response = openai_client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {
                    "role": "system",
                    "content": "ë„ˆëŠ” ì˜ë„ ë¶„ë¥˜ ì „ë¬¸ê°€ì•¼. ì •í™•í•˜ê²Œ ë¶„ë¥˜í•´ì¤˜.",
                },
                {"role": "user", "content": prompt},
            ],
            temperature=0,
        )

        return response.choices[0].message.content.strip().lower()

    loop = asyncio.get_event_loop()
    return await loop.run_in_executor(executor, sync_determine_intent)


async def extract_photo_keywords(query: str) -> List[str]:
    """ì‚¬ì§„ ê²€ìƒ‰ì„ ìœ„í•œ í‚¤ì›Œë“œ ì¶”ì¶œ - ê°œì„ ëœ ë²„ì „"""

    def sync_extract_keywords():
        prompt = f"""
ë‹¤ìŒ ì§ˆë¬¸ì—ì„œ ì‚¬ì§„ì„ ì°¾ê¸° ìœ„í•œ ëª¨ë“  ê´€ë ¨ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ì„¸ìš”.
ë‚ ì§œ/ì‹œê°„ í‚¤ì›Œë“œëŠ” ì œì™¸í•˜ê³ , ì‚¬ì§„ì˜ ë‚´ìš©ì„ ë‚˜íƒ€ë‚´ëŠ” í•µì‹¬ í‚¤ì›Œë“œë§Œ ì¶”ì¶œí•©ë‹ˆë‹¤.

ì§ˆë¬¸: {query}

ì¹´í…Œê³ ë¦¬ë³„ í‚¤ì›Œë“œ:
1. ì¥ì†Œ: ì§‘, íšŒì‚¬, ì¹´í˜, ê³µì›, ë°”ë‹¤, ì‚°, ì‹ë‹¹, í•™êµ, í˜¸í…”, ê±°ë¦¬, ë§¤ì¥
2. í™œë™: ì—¬í–‰, íŒŒí‹°, íšŒì˜, ì‹ì‚¬, ìš´ë™, ì‚°ì±…, ì‡¼í•‘, ë†€ì´, í–‰ì‚¬, ì¶•ì œ
3. ì‚¬ëŒ: ê°€ì¡±, ì¹œêµ¬, ë™ë£Œ, ì• ì™„ë™ë¬¼, ì—°ì¸, ë¶€ëª¨ë‹˜, ì•„ì´, ì†ë‹˜
4. ì‚¬ë¬¼: ìŒì‹, ì°¨, ê±´ë¬¼, í’ê²½, ê½ƒ, ì„ ë¬¼, ì¼€ì´í¬, ë¬¸ì„œ, ì œí’ˆ
5. ìƒí™©: ìƒì¼, ê¸°ë…ì¼, íœ´ê°€, ì¶œì¥, ëª¨ì„, ë°ì´íŠ¸, ê²°í˜¼ì‹

ì¶”ì¶œ ê·œì¹™:
1. ë‚ ì§œ/ì‹œê°„ í‘œí˜„ ì œì™¸
2. ë™ì‚¬ëŠ” ëª…ì‚¬ë¡œ ë³€í™˜
3. ê°€ëŠ¥í•œ ë§ì€ ì—°ê´€ í‚¤ì›Œë“œ í¬í•¨
4. ë¬¸ë§¥ìƒ ì•”ì‹œëœ í‚¤ì›Œë“œë„ ì¶”ê°€

ì˜ˆì‹œ:
"ìƒì¼ íŒŒí‹° ì‚¬ì§„" â†’ ["ìƒì¼", "íŒŒí‹°", "ì¶•í•˜", "ì¼€ì´í¬", "ì¹œêµ¬", "ëª¨ì„", "ê¸°ë…ì¼"]
"íšŒì‚¬ì—ì„œ ì°ì€ ì‚¬ì§„" â†’ ["íšŒì‚¬", "ì‚¬ë¬´ì‹¤", "ì§ì¥", "ë™ë£Œ", "ì—…ë¬´", "ë¯¸íŒ…"]
"ì—¬í–‰ ê°€ì„œ ì°ì€ í’ê²½" â†’ ["ì—¬í–‰", "í’ê²½", "ê´€ê´‘", "íœ´ê°€", "ìì—°", "ê²½ì¹˜"]

JSON ë°°ì—´ë¡œë§Œ ë°˜í™˜í•˜ì„¸ìš”.
"""

        response = openai_client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {
                    "role": "system",
                    "content": "í‚¤ì›Œë“œ ì¶”ì¶œ ì „ë¬¸ê°€. ì‚¬ì§„ ê²€ìƒ‰ì— ë„ì›€ë˜ëŠ” ëª¨ë“  í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•´. JSONë§Œ ë°˜í™˜.",
                },
                {"role": "user", "content": prompt},
            ],
            temperature=0.3,
        )

        keywords_raw = response.choices[0].message.content

        # ì½”ë“œë¸”ë¡ ì œê±°
        if "```" in keywords_raw:
            keywords_raw = (
                keywords_raw.replace("```json", "").replace("```", "").strip()
            )

        try:
            keywords = json.loads(keywords_raw)

            # í‚¤ì›Œë“œ í™•ì¥
            expanded_keywords = set(keywords)

            for keyword in keywords:
                # í™œë™ ê´€ë ¨ í™•ì¥
                if keyword in ["ì—¬í–‰", "ê´€ê´‘", "íœ´ê°€"]:
                    expanded_keywords.update(["ë‚˜ë“¤ì´", "ì™¸ì¶œ", "íƒë°©"])
                elif keyword in ["íŒŒí‹°", "ëª¨ì„"]:
                    expanded_keywords.update(["í–‰ì‚¬", "ì¶•í•˜", "ê¸°ë…"])
                elif keyword in ["ì‹ì‚¬", "ìŒì‹"]:
                    expanded_keywords.update(["ë§›ì§‘", "ìš”ë¦¬", "ì™¸ì‹"])

                # ì¥ì†Œ ê´€ë ¨ í™•ì¥
                if keyword == "íšŒì‚¬":
                    expanded_keywords.update(["ì‚¬ë¬´ì‹¤", "ì§ì¥", "ì—…ë¬´"])
                elif keyword == "ì§‘":
                    expanded_keywords.update(["í™ˆ", "ê±°ì‹¤", "ë°©"])
                elif keyword in ["ë°”ë‹¤", "í•´ë³€"]:
                    expanded_keywords.update(["í•´ì•ˆ", "ë°”ë‹·ê°€", "í•´ìˆ˜ìš•ì¥"])

                # ì‚¬ëŒ ê´€ë ¨ í™•ì¥
                if keyword == "ê°€ì¡±":
                    expanded_keywords.update(["ë¶€ëª¨", "í˜•ì œ", "ì¹œì²™"])
                elif keyword == "ì¹œêµ¬":
                    expanded_keywords.update(["ë™ë£Œ", "ì¹œëª©", "ìš°ì •"])

            return list(expanded_keywords)

        except json.JSONDecodeError:
            return [
                kw.strip() for kw in keywords_raw.strip("[]").split(",") if kw.strip()
            ]

    loop = asyncio.get_event_loop()
    return await loop.run_in_executor(executor, sync_extract_keywords)


async def search_photos_by_keywords(user_id: str, keywords: List[str]) -> List[Dict]:
    """í‚¤ì›Œë“œë¡œ DBì—ì„œ ì‚¬ì§„ ê²€ìƒ‰ - ë§¤ì¹˜ ì ìˆ˜ í¬í•¨"""

    def sync_search_photos():
        connection = psycopg2.connect(**DB_PARAMS)
        cursor = connection.cursor()

        try:
            # ê°œì„ ëœ ê²€ìƒ‰ ì¿¼ë¦¬ - ë§¤ì¹˜ ì ìˆ˜ì™€ ì •ë³´ í¬í•¨
            query = """
            WITH keyword_matches AS (
                SELECT 
                    i.access_id,
                    i.image_time,
                    i.caption,
                    ik.keyword,
                    CASE 
                        WHEN ik.keyword = ANY(%s) THEN 1.0
                        ELSE 0.5
                    END as match_score
                FROM images i
                JOIN image_keywords ik ON i.id = ik.image_id
                WHERE i.user_id = %s 
                AND (
                    ik.keyword = ANY(%s)
                    OR EXISTS (
                        SELECT 1 FROM unnest(%s::text[]) AS search_kw
                        WHERE ik.keyword ILIKE '%%' || search_kw || '%%'
                    )
                )
            ),
            aggregated AS (
                SELECT 
                    access_id,
                    image_time,
                    caption,
                    COUNT(DISTINCT keyword) as keyword_count,
                    SUM(match_score) as total_score,
                    STRING_AGG(DISTINCT keyword, ', ') as matched_keywords
                FROM keyword_matches
                GROUP BY access_id, image_time, caption
            )
            SELECT 
                access_id,
                caption,
                keyword_count,
                total_score,
                matched_keywords,
                image_time
            FROM aggregated
            ORDER BY total_score DESC, keyword_count DESC, image_time DESC
            LIMIT 30;
            """

            cursor.execute(query, (keywords, user_id, keywords, keywords))
            results = cursor.fetchall()

            # ê²°ê³¼ë¥¼ ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
            photo_results = []
            for row in results[:10]:  # ìƒìœ„ 10ê°œë§Œ
                photo_results.append(
                    {
                        "access_id": row[0],
                        "caption": row[1],
                        "match_count": row[2],
                        "score": float(row[3]),
                        "matched_keywords": row[4],
                        "image_time": row[5].isoformat() if row[5] else None,
                    }
                )

                logger.info(
                    f"ğŸ“· ì‚¬ì§„: {row[0]}, ì ìˆ˜: {row[3]:.1f}, ë§¤ì¹˜: {row[2]}ê°œ, í‚¤ì›Œë“œ: {row[4]}"
                )

            return photo_results

        finally:
            cursor.close()
            connection.close()

    loop = asyncio.get_event_loop()
    return await loop.run_in_executor(executor, sync_search_photos)


async def expand_info_query(query: str) -> List[str]:
    """ì •ë³´ ê²€ìƒ‰ì„ ìœ„í•œ ì¿¼ë¦¬ í™•ì¥"""

    def sync_expand_query():
        prompt = f"""
ë‹¤ìŒ ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë‹¤ì–‘í•œ ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ ìƒì„±í•˜ì„¸ìš”.
ì›ë³¸ ì§ˆë¬¸ì˜ ì˜ë¯¸ë¥¼ ìœ ì§€í•˜ë©´ì„œ ë™ì˜ì–´, ê´€ë ¨ì–´, ë‹¤ì–‘í•œ í‘œí˜„ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.

ì›ë³¸ ì§ˆë¬¸: {query}

ë³€í˜• ê·œì¹™:
1. í•µì‹¬ í‚¤ì›Œë“œ í¬í•¨
2. ë™ì˜ì–´ ì‚¬ìš©
3. ì•½ì–´ì™€ ì „ì²´ í‘œí˜„
4. í•œêµ­ì–´ì™€ ì˜ì–´ í˜¼ìš©
5. ê´€ë ¨ ìš©ì–´ ì¶”ê°€

ì˜ˆì‹œ:
"ìŠ¤íƒ€ë²…ìŠ¤ ì™€ì´íŒŒì´ ì•Œë ¤ì¤˜" â†’ 
["ìŠ¤íƒ€ë²…ìŠ¤ WiFi", "ìŠ¤íƒ€ë²…ìŠ¤ ì™€ì´íŒŒì´", "starbucks wifi password", "ìŠ¤íƒ€ë²…ìŠ¤ ì¸í„°ë„·", "ìŠ¤íƒ€ë²…ìŠ¤ ë¬´ì„ ì¸í„°ë„·", "ìŠ¤íƒ€ë²…ìŠ¤ ë¹„ë°€ë²ˆí˜¸", "KT WiFi zone", "ìŠ¤íƒ€ë²…ìŠ¤ ë„¤íŠ¸ì›Œí¬"]

JSON ë°°ì—´ë¡œ 5-7ê°œì˜ ë³€í˜• ì¿¼ë¦¬ë¥¼ ë°˜í™˜í•˜ì„¸ìš”.
"""

        response = openai_client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "ì¿¼ë¦¬ í™•ì¥ ì „ë¬¸ê°€. JSON ë°°ì—´ë§Œ ë°˜í™˜."},
                {"role": "user", "content": prompt},
            ],
            temperature=0.3,
        )

        queries_raw = response.choices[0].message.content

        # ì½”ë“œë¸”ë¡ ì œê±°
        if "```" in queries_raw:
            queries_raw = queries_raw.replace("```json", "").replace("```", "").strip()

        try:
            expanded = json.loads(queries_raw)
            expanded.append(query)  # ì›ë³¸ ì¿¼ë¦¬ í¬í•¨
            return list(set(expanded))[:8]
        except:
            return [query]

    loop = asyncio.get_event_loop()
    return await loop.run_in_executor(executor, sync_expand_query)


async def generate_info_answer(
    user_id: str, query: str, context_info: List[Dict]
) -> str:
    """ì •ë³´ ê¸°ë°˜ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ìƒì„±"""

    def sync_generate_answer():
        # context ì •ë³´ë¥¼ ë” ì²´ê³„ì ìœ¼ë¡œ ì •ë¦¬
        context_texts = []
        for i, item in enumerate(context_info[:5]):  # ìƒìœ„ 5ê°œë§Œ ì‚¬ìš©
            text = item.get("text", "").strip()
            if text:
                context_texts.append(f"{i+1}. {text}")

        context_text = "\n".join(context_texts)

        if not context_text:
            return "ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì§ˆë¬¸ì„ í•´ë³´ì„¸ìš”."

        prompt = f"""
ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì•„ë˜ ì œê³µëœ ì •ë³´ë¥¼ í™œìš©í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”.
ì •ë³´ê°€ ë¶€ì¡±í•˜ë‹¤ë©´ ê·¸ ì‚¬ì‹¤ì„ ì–¸ê¸‰í•˜ê³ , ì•Œë ¤ì§„ ë‚´ìš©ë§Œìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”.

[ì œê³µëœ ì •ë³´]
{context_text}

[ì‚¬ìš©ì ì§ˆë¬¸]
{query}

ë‹µë³€ ì‘ì„± ê·œì¹™:
1. ì œê³µëœ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì •í™•í•˜ê²Œ ë‹µë³€
2. ì¹œê·¼í•˜ê³  ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€í™”ì²´ ì‚¬ìš©
3. ë¶ˆí™•ì‹¤í•œ ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ê³  ëª…ì‹œ
4. í•„ìš”ì‹œ ì¶”ê°€ ì •ë³´ë¥¼ ìš”ì²­í•˜ëŠ” ì•ˆë‚´ í¬í•¨

ë‹µë³€:
"""

        response = openai_client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {
                    "role": "system",
                    "content": "ë„ˆëŠ” ì‚¬ìš©ìì˜ ê°œì¸ ë¹„ì„œì•¼. ì œê³µëœ ì •ë³´ë¥¼ í™œìš©í•´ ì •í™•í•˜ê³  ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ í•´ì¤˜.",
                },
                {"role": "user", "content": prompt},
            ],
            temperature=0.7,
        )

        return response.choices[0].message.content

    loop = asyncio.get_event_loop()
    return await loop.run_in_executor(executor, sync_generate_answer)


async def save_query_async(user_id: str, role: str, content: str, timestamp: int):
    """ì¿¼ë¦¬ ì €ì¥ì„ ìœ„í•œ ë¹„ë™ê¸° ë˜í¼"""
    try:
        loop = asyncio.get_event_loop()
        await loop.run_in_executor(
            executor, save_chat_vector_to_pinecone, user_id, role, content, timestamp
        )
        logger.info(f"âœ… ì¿¼ë¦¬ ì €ì¥ ì™„ë£Œ: {user_id}")
    except Exception as e:
        logger.error(f"âŒ ì¿¼ë¦¬ ì €ì¥ ì‹¤íŒ¨: {user_id} - {str(e)}")


async def save_result_async(user_id: str, role: str, content: str, timestamp: int):
    """ê²°ê³¼ ì €ì¥ì„ ìœ„í•œ ë¹„ë™ê¸° ë˜í¼"""
    try:
        loop = asyncio.get_event_loop()
        await loop.run_in_executor(
            executor, save_chat_vector_to_pinecone, user_id, role, content, timestamp
        )
        logger.info(f"âœ… ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {user_id}")
    except Exception as e:
        logger.error(f"âŒ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {user_id} - {str(e)}")


@router.post("/image/")
async def process_image_query(user_id: str = Form(...), query: str = Form(...)):
    """ì´ë¯¸ì§€ ê´€ë ¨ ì§ˆë¬¸ ì²˜ë¦¬ API"""
    total_start = time.time()

    logger.info(f"ğŸ” ì´ë¯¸ì§€ ì¿¼ë¦¬ ì‹œì‘ - user: {user_id}, query: {query}")

    # ìºì‹œ í™•ì¸
    cache_key = get_cache_key(user_id, query)
    cached_result = get_from_cache(cache_key)
    if cached_result:
        cached_result["_timings"]["total"] = time.time() - total_start
        cached_result["_from_cache"] = True
        return cached_result

    timestamp = int(time.time())
    timings = {}

    try:
        # 1. ì¿¼ë¦¬ ì €ì¥ (ë¹„ë™ê¸°)
        asyncio.create_task(save_query_async(user_id, "user", query, timestamp))

        # 2. ì˜ë„ íŒŒì•…
        intent_start = time.time()
        intent = await determine_image_query_intent(query)
        timings["intent_detection"] = time.time() - intent_start
        logger.info(f"ğŸ¯ ì˜ë„ íŒŒì•…: {intent} ({timings['intent_detection']:.3f}ì´ˆ)")

        if intent == "find_photo":
            # ì‚¬ì§„ ì°¾ê¸° ë¡œì§

            # 3-1. í‚¤ì›Œë“œ ì¶”ì¶œ
            keyword_start = time.time()
            keywords = await extract_photo_keywords(query)
            timings["keyword_extraction"] = time.time() - keyword_start
            logger.info(
                f"ğŸ” ì¶”ì¶œëœ í‚¤ì›Œë“œ ({len(keywords)}ê°œ): {keywords[:10]} ({timings['keyword_extraction']:.3f}ì´ˆ)"
            )

            # 4-1. DBì—ì„œ ì‚¬ì§„ ê²€ìƒ‰
            db_search_start = time.time()
            photo_results = await search_photos_by_keywords(user_id, keywords)
            timings["db_search"] = time.time() - db_search_start

            # photo_idsë§Œ ì¶”ì¶œ
            photo_ids = [photo["access_id"] for photo in photo_results]

            logger.info(
                f"ğŸ“· ê²€ìƒ‰ëœ ì‚¬ì§„: {len(photo_ids)}ê°œ ({timings['db_search']:.3f}ì´ˆ)"
            )

            # ê²°ê³¼ êµ¬ì„±
            result = {
                "type": "photo_search",
                "query": query,
                "keywords": keywords,
                "photo_ids": photo_ids,
                "photo_details": photo_results[:5],  # ìƒìœ„ 5ê°œ ìƒì„¸ ì •ë³´
                "answer": "",
                "count": len(photo_ids),
                "_timings": timings,
            }

        else:  # get_info
            # ì •ë³´ ì°¾ê¸° ë¡œì§

            # 3-2. ì¿¼ë¦¬ í™•ì¥
            query_expand_start = time.time()
            expanded_queries = await expand_info_query(query)
            timings["query_expansion"] = time.time() - query_expand_start
            logger.info(f"ğŸ” í™•ì¥ëœ ì¿¼ë¦¬: {expanded_queries}")

            # 3-3. ë²¡í„° ê²€ìƒ‰ìœ¼ë¡œ ê´€ë ¨ ì •ë³´ ì°¾ê¸°
            vector_search_start = time.time()

            # namespaceë¥¼ user_id_informationìœ¼ë¡œ ì„¤ì •
            namespace = f"{user_id}_information"

            loop = asyncio.get_event_loop()

            logger.info(
                f"ğŸ” ë²¡í„° ê²€ìƒ‰ ì‹œì‘ - namespace: {namespace}, queries: {expanded_queries}"
            )

            # ë¨¼ì € ì›ë³¸ ì¿¼ë¦¬ë¡œ ê²€ìƒ‰
            context_info = []
            try:
                # 1. í™•ì¥ëœ ì¿¼ë¦¬ë¡œ ê²€ìƒ‰
                result1 = await loop.run_in_executor(
                    executor,
                    search_similar_items_enhanced_optimized,
                    user_id,
                    expanded_queries,
                    "info",
                    20,
                )
                context_info.extend(result1)
                logger.info(f"âœ… í™•ì¥ëœ ì¿¼ë¦¬ ê²°ê³¼: {len(result1)}ê°œ")

                # 2. ì›ë³¸ ì¿¼ë¦¬ë¡œë„ ê²€ìƒ‰
                if len(context_info) < 5:
                    result2 = await loop.run_in_executor(
                        executor,
                        search_similar_items_enhanced_optimized,
                        user_id,
                        [query],
                        "info",
                        10,
                    )
                    context_info.extend(result2)
                    logger.info(f"âœ… ì›ë³¸ ì¿¼ë¦¬ ê²°ê³¼: {len(result2)}ê°œ")

                # ì¤‘ë³µ ì œê±°
                seen_texts = set()
                unique_results = []
                for item in context_info:
                    text = item.get("text", "")
                    if text and text not in seen_texts:
                        seen_texts.add(text)
                        unique_results.append(item)

                context_info = unique_results[:20]

                # ê²€ìƒ‰ ê²°ê³¼ ë””ë²„ê¹…
                if context_info:
                    logger.info(f"ğŸ” ê²€ìƒ‰ ê²°ê³¼ ìƒ˜í”Œ:")
                    for i, info in enumerate(context_info[:3]):
                        logger.info(f"  {i+1}. {info.get('text', '')[:100]}...")
                else:
                    logger.warning(f"âš ï¸ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ - namespace: {namespace}")

            except Exception as e:
                logger.error(f"âŒ ë²¡í„° ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}", exc_info=True)
                context_info = []

            timings["vector_search"] = time.time() - vector_search_start
            logger.info(
                f"ğŸ“š ê²€ìƒ‰ëœ ì •ë³´: {len(context_info)}ê°œ ({timings['vector_search']:.3f}ì´ˆ)"
            )

            # 4-2. ë‹µë³€ ìƒì„±
            answer_start = time.time()
            answer = await generate_info_answer(user_id, query, context_info)
            timings["answer_generation"] = time.time() - answer_start
            logger.info(f"âœï¸ ë‹µë³€ ìƒì„± ì™„ë£Œ ({timings['answer_generation']:.3f}ì´ˆ)")

            # ê²°ê³¼ êµ¬ì„±
            result = {
                "type": "info_search",
                "query": query,
                "answer": answer,
                "context_count": len(context_info),
                "photo_ids": [],
                "_timings": timings,
                "_debug": {
                    "expanded_queries": expanded_queries,
                    "namespace": namespace,
                    "context_sample": (
                        context_info[0].get("text", "")[:200] if context_info else None
                    ),
                },
            }

        # ì „ì²´ ì‹œê°„
        timings["total"] = time.time() - total_start
        result["_timings"] = timings
        result["_from_cache"] = False

        # ìºì‹œì— ì €ì¥
        set_cache(cache_key, result)

        # ê²°ê³¼ ì €ì¥ (ë¹„ë™ê¸°)
        asyncio.create_task(
            save_result_async(
                user_id,
                "assistant",
                json.dumps(result, ensure_ascii=False),
                int(time.time()),
            )
        )

        logger.info(
            f"""
â±ï¸ Image API ì„±ëŠ¥ ìš”ì•½:
- ì˜ë„ íŒŒì•…: {timings['intent_detection']:.3f}ì´ˆ
- {'í‚¤ì›Œë“œ ì¶”ì¶œ' if intent == 'find_photo' else 'ì¿¼ë¦¬ í™•ì¥'}: {timings.get('keyword_extraction', timings.get('query_expansion', 0)):.3f}ì´ˆ
- {'DB ê²€ìƒ‰' if intent == 'find_photo' else 'ë²¡í„° ê²€ìƒ‰'}: {timings.get('db_search', timings.get('vector_search', 0)):.3f}ì´ˆ
- {'DB ê²€ìƒ‰' if intent == 'find_photo' else 'ë‹µë³€ ìƒì„±'}: {timings.get('db_search', timings.get('answer_generation', 0)):.3f}ì´ˆ
- ì „ì²´ ì‹œê°„: {timings['total']:.3f}ì´ˆ
        """
        )

        return result

    except Exception as e:
        error_time = time.time() - total_start
        logger.error(
            f"âŒ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ ({error_time:.3f}ì´ˆ): {str(e)}", exc_info=True
        )
        return {
            "error": "ìš”ì²­ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
            "detail": str(e),
            "timings": timings,
            "processing_time": f"{error_time:.3f}ì´ˆ",
        }


# ìºì‹œ ê´€ë¦¬ ì—”ë“œí¬ì¸íŠ¸
@router.get("/image/cache/status")
async def get_cache_status():
    """ìºì‹œ ìƒíƒœ í™•ì¸"""
    current_time = datetime.now()
    valid_count = sum(1 for data in cache.values() if current_time < data["expires_at"])
    expired_count = len(cache) - valid_count

    return {
        "total_items": len(cache),
        "valid_items": valid_count,
        "expired_items": expired_count,
        "max_size": MAX_CACHE_SIZE,
        "ttl_seconds": CACHE_TTL_SECONDS,
    }


@router.delete("/image/cache/clear")
async def clear_cache():
    """ìºì‹œ ì´ˆê¸°í™”"""
    cache_size = len(cache)
    cache.clear()
    logger.info(f"ğŸ—‘ï¸ ìºì‹œ ì´ˆê¸°í™”: {cache_size}ê°œ í•­ëª© ì‚­ì œ")
    return {"cleared_items": cache_size}


# ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•œ ë””ë²„ê·¸ ì—”ë“œí¬ì¸íŠ¸
@router.post("/image/debug-search")
async def debug_search(
    user_id: str = Form(...),
    query: str = Form(...),
    namespace: Optional[str] = Form(None),
):
    """ê²€ìƒ‰ ë””ë²„ê¹…ì„ ìœ„í•œ ì—”ë“œí¬ì¸íŠ¸"""
    try:
        # namespaceê°€ ì œê³µë˜ì§€ ì•Šìœ¼ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©
        if namespace is None:
            namespace = f"{user_id}_information"

        logger.info(
            f"ğŸ” ë””ë²„ê·¸ ê²€ìƒ‰ - user: {user_id}, query: {query}, namespace: {namespace}"
        )

        # ì¿¼ë¦¬ í™•ì¥
        expanded_queries = await expand_info_query(query)
        logger.info(f"ğŸ” í™•ì¥ëœ ì¿¼ë¦¬: {expanded_queries}")

        loop = asyncio.get_event_loop()

        # ì—¬ëŸ¬ ë°©ë²•ìœ¼ë¡œ ê²€ìƒ‰ ì‹œë„
        all_results = []

        # 1. í™•ì¥ëœ ì¿¼ë¦¬ë¡œ ê²€ìƒ‰
        logger.info(f"ğŸ¯ í™•ì¥ëœ ì¿¼ë¦¬ë¡œ ê²€ìƒ‰ ì‹œë„...")
        result1 = await loop.run_in_executor(
            executor,
            search_similar_items_enhanced_optimized,
            user_id,
            expanded_queries,
            "info",
            20,
        )
        all_results.extend(result1)
        logger.info(f"âœ… í™•ì¥ ì¿¼ë¦¬ ê²°ê³¼: {len(result1)}ê°œ")

        # 2. ì›ë³¸ ì¿¼ë¦¬ë¡œë§Œ ê²€ìƒ‰
        logger.info(f"ğŸ¯ ì›ë³¸ ì¿¼ë¦¬ë¡œ ê²€ìƒ‰ ì‹œë„...")
        result2 = await loop.run_in_executor(
            executor,
            search_similar_items_enhanced_optimized,
            user_id,
            [query],
            "info",
            20,
        )
        all_results.extend(result2)
        logger.info(f"âœ… ì›ë³¸ ì¿¼ë¦¬ ê²°ê³¼: {len(result2)}ê°œ")

        # 3. ë‹¨ìˆœ í‚¤ì›Œë“œë¡œ ê²€ìƒ‰
        keywords = query.split()
        logger.info(f"ğŸ¯ í‚¤ì›Œë“œë¡œ ê²€ìƒ‰ ì‹œë„: {keywords}")
        result3 = await loop.run_in_executor(
            executor,
            search_similar_items_enhanced_optimized,
            user_id,
            keywords,
            "info",
            20,
        )
        all_results.extend(result3)
        logger.info(f"âœ… í‚¤ì›Œë“œ ê²°ê³¼: {len(result3)}ê°œ")

        # ì¤‘ë³µ ì œê±°
        unique_results = []
        seen_texts = set()
        for result in all_results:
            text = result.get("text", "")
            if text and text not in seen_texts:
                seen_texts.add(text)
                unique_results.append(result)

        return {
            "query": query,
            "expanded_queries": expanded_queries,
            "namespace": namespace,
            "results_count": len(unique_results),
            "results_breakdown": {
                "expanded_queries_count": len(result1),
                "original_query_count": len(result2),
                "keywords_count": len(result3),
            },
            "results": [
                {
                    "text": result.get("text", "")[:200] + "...",
                    "score": result.get("score", 0),
                    "metadata": result.get("metadata", {}),
                }
                for result in unique_results[:5]
            ],
            "debug_info": {
                "user_id": user_id,
                "namespace_used": namespace,
                "keywords_tried": keywords,
            },
        }
    except Exception as e:
        logger.error(f"âŒ ë””ë²„ê·¸ ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}", exc_info=True)
        return {
            "error": str(e),
            "query": query,
            "namespace": namespace,
            "traceback": traceback.format_exc(),
        }
