from fastapi import APIRouter, Form
from typing import Optional
from app.utils.semantic_search import (
    enhance_query_with_personal_context_v2,
    determine_query_intent,
    search_similar_items_enhanced,
    search_similar_items_enhanced_optimized,
    filter_relevant_items_with_context,
    generate_answer_by_intent,
)
from app.utils.chat_vector_store import save_chat_vector_to_pinecone
import json, time
import asyncio
from concurrent.futures import ThreadPoolExecutor
import logging
import hashlib
from datetime import datetime, timedelta
from typing import Dict, Optional

router = APIRouter()
logger = logging.getLogger(__name__)

# ThreadPoolExecutor ì›Œì»¤ ìˆ˜ ì¦ê°€
executor = ThreadPoolExecutor(max_workers=20)  # ê¸°ì¡´ 5ì—ì„œ 20ìœ¼ë¡œ ì¦ê°€

# ê°„ë‹¨í•œ ë©”ëª¨ë¦¬ ìºì‹œ
cache: Dict[str, Dict] = {}
CACHE_TTL_SECONDS = 3600  # 1ì‹œê°„
MAX_CACHE_SIZE = 1000  # ìµœëŒ€ ìºì‹œ í¬ê¸°


def get_cache_key(user_id: str, query: str, top_k_photo: int, top_k_info: int) -> str:
    """ìºì‹œ í‚¤ ìƒì„±"""
    cache_data = f"answer:{user_id}:{query}:{top_k_photo}:{top_k_info}"
    return hashlib.md5(cache_data.encode()).hexdigest()


def get_from_cache(key: str) -> Optional[Dict]:
    """ìºì‹œì—ì„œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°"""
    if key in cache:
        cached_data = cache[key]
        # TTL í™•ì¸
        if datetime.now() < cached_data["expires_at"]:
            logger.info(f"âœ… ìºì‹œ íˆíŠ¸: {key}")
            return cached_data["data"]
        else:
            # ë§Œë£Œëœ ìºì‹œ ì‚­ì œ
            del cache[key]
            logger.info(f"ğŸ—‘ï¸ ë§Œë£Œëœ ìºì‹œ ì‚­ì œ: {key}")
    return None


def set_cache(key: str, data: Dict):
    """ìºì‹œì— ë°ì´í„° ì €ì¥"""
    # ìºì‹œ í¬ê¸° ì œí•œ - LRU ë°©ì‹ìœ¼ë¡œ ì˜¤ë˜ëœ í•­ëª© ì œê±°
    if len(cache) >= MAX_CACHE_SIZE:
        # ê°€ì¥ ì˜¤ë˜ëœ í•­ëª© ì°¾ì•„ì„œ ì œê±°
        oldest_key = min(cache.keys(), key=lambda k: cache[k]["created_at"])
        del cache[oldest_key]
        logger.info(f"ğŸ—‘ï¸ ìºì‹œ í¬ê¸° ì´ˆê³¼ë¡œ ì˜¤ë˜ëœ í•­ëª© ì œê±°: {oldest_key}")

    cache[key] = {
        "data": data,
        "created_at": datetime.now(),
        "expires_at": datetime.now() + timedelta(seconds=CACHE_TTL_SECONDS),
    }
    logger.info(f"ğŸ’¾ ìºì‹œ ì €ì¥: {key}")


# ë¹„ë™ê¸° ì €ì¥ í•¨ìˆ˜ë“¤
async def _save_query_async(user_id: str, role: str, content: str, timestamp: int):
    """Async wrapper for saving query"""
    loop = asyncio.get_event_loop()
    await loop.run_in_executor(
        executor, save_chat_vector_to_pinecone, user_id, role, content, timestamp
    )


async def _save_result_async(user_id: str, role: str, content: str, timestamp: int):
    """Async wrapper for saving result"""
    try:
        loop = asyncio.get_event_loop()
        await loop.run_in_executor(
            executor, save_chat_vector_to_pinecone, user_id, role, content, timestamp
        )
        logger.info(f"âœ… ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {user_id}")
    except Exception as e:
        logger.error(f"âŒ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {user_id} - {str(e)}")


@router.post("/answer/")
async def search(
    user_id: str = Form(...),
    query: str = Form(...),
    top_k_photo: Optional[int] = Form(5),
    top_k_info: Optional[int] = Form(5),
):
    # ì „ì²´ ì‹œì‘ ì‹œê°„
    total_start = time.time()

    # ìºì‹œ í™•ì¸
    cache_key = get_cache_key(user_id, query, top_k_photo, top_k_info)
    cached_result = get_from_cache(cache_key)
    if cached_result:
        cached_result["_timings"]["total"] = time.time() - total_start
        cached_result["_from_cache"] = True
        return cached_result

    timestamp = int(time.time())
    loop = asyncio.get_event_loop()

    # ê° ë‹¨ê³„ë³„ ì‹œê°„ ê¸°ë¡ìš© ë”•ì…”ë„ˆë¦¬
    timings = {}

    try:
        # 1. ì‚¬ìš©ì ì¿¼ë¦¬ ì €ì¥ (ì™„ì „ ë¹„ë™ê¸°)
        asyncio.create_task(_save_query_async(user_id, "user", query, timestamp))

        # 2. ì¿¼ë¦¬ í™•ì¥
        expand_start = time.time()
        expanded_queries = await loop.run_in_executor(
            executor, enhance_query_with_personal_context_v2, user_id, query
        )
        timings["query_expansion"] = time.time() - expand_start
        logger.info(f"â±ï¸ ì¿¼ë¦¬ í™•ì¥: {timings['query_expansion']:.3f}ì´ˆ")
        logger.info(f"ğŸ” ì˜ë¯¸ ê¸°ë°˜ í™•ì¥ ì¿¼ë¦¬ (ì „ì²´): {expanded_queries}")
        logger.info(f"ğŸ” ì˜ë¯¸ ê¸°ë°˜ í™•ì¥ ì¿¼ë¦¬ (Top 3): {expanded_queries[:3]}")

        # 3. ì§ˆë¬¸ ì˜ë„ íŒŒì•…
        intent_start = time.time()
        query_intent = determine_query_intent(query)
        timings["intent_detection"] = time.time() - intent_start
        logger.info(f"â±ï¸ ì˜ë„ íŒŒì•…: {timings['intent_detection']:.3f}ì´ˆ")

        # 4. ë²¡í„° ê²€ìƒ‰ (ë³‘ë ¬)
        vector_search_start = time.time()
        info_search_task = loop.run_in_executor(
            executor,
            search_similar_items_enhanced_optimized,
            user_id,
            expanded_queries,
            "info",
            top_k_info,
        )

        photo_search_task = loop.run_in_executor(
            executor,
            search_similar_items_enhanced_optimized,
            user_id,
            expanded_queries,
            "photo",
            top_k_photo,
        )

        raw_info_results, raw_photo_results = await asyncio.gather(
            info_search_task, photo_search_task
        )
        timings["vector_search"] = time.time() - vector_search_start
        logger.info(f"â±ï¸ ë²¡í„° ê²€ìƒ‰ (ë³‘ë ¬): {timings['vector_search']:.3f}ì´ˆ")

        # 5. ê²°ê³¼ í•„í„°ë§ (ë³‘ë ¬)
        filter_start = time.time()
        info_filter_task = loop.run_in_executor(
            executor,
            filter_relevant_items_with_context,
            query,
            "",
            raw_info_results,
            "ì •ë³´",
        )

        photo_filter_task = loop.run_in_executor(
            executor,
            filter_relevant_items_with_context,
            query,
            "",
            raw_photo_results,
            "ì‚¬ì§„",
        )

        info_results, photo_results = await asyncio.gather(
            info_filter_task, photo_filter_task
        )
        timings["filtering"] = time.time() - filter_start
        logger.info(f"â±ï¸ ê²°ê³¼ í•„í„°ë§ (ë³‘ë ¬): {timings['filtering']:.3f}ì´ˆ")

        # 6. ë‹µë³€ ìƒì„±
        answer_start = time.time()
        result = await loop.run_in_executor(
            executor,
            generate_answer_by_intent,
            user_id,
            query,
            info_results,
            photo_results,
            query_intent,
        )
        timings["answer_generation"] = time.time() - answer_start
        logger.info(f"â±ï¸ ë‹µë³€ ìƒì„±: {timings['answer_generation']:.3f}ì´ˆ")

        # ì „ì²´ ì‹œê°„ (ì‚¬ìš©ì ì‘ë‹µ ì‹œì )
        timings["total"] = time.time() - total_start

        # ê²°ê³¼ì— íƒ€ì´ë° ì •ë³´ í¬í•¨ (ë””ë²„ê¹…ìš©)
        result["_timings"] = timings
        result["_from_cache"] = False

        # ìºì‹œì— ì €ì¥
        set_cache(cache_key, result)

        # 7. ê²°ê³¼ ì €ì¥ (ì‘ë‹µ í›„ ë¹„ë™ê¸°ë¡œ ì²˜ë¦¬)
        serialized_result = json.dumps(result, ensure_ascii=False)
        asyncio.create_task(
            _save_result_async(
                user_id, "assistant", serialized_result, int(time.time())
            )
        )

        # ìš”ì•½ ë¡œê·¸
        logger.info(
            f"""
â±ï¸ ê²€ìƒ‰ API ì„±ëŠ¥ ìš”ì•½:
- ì¿¼ë¦¬ í™•ì¥: {timings['query_expansion']:.3f}ì´ˆ
- ì˜ë„ íŒŒì•…: {timings['intent_detection']:.3f}ì´ˆ
- ë²¡í„° ê²€ìƒ‰: {timings['vector_search']:.3f}ì´ˆ
- ê²°ê³¼ í•„í„°ë§: {timings['filtering']:.3f}ì´ˆ
- ë‹µë³€ ìƒì„±: {timings['answer_generation']:.3f}ì´ˆ
- ì „ì²´ ì‹œê°„: {timings['total']:.3f}ì´ˆ (ì‘ë‹µ ì‹œì )
        """
        )

        # ì‘ë‹µ ì¦‰ì‹œ ë°˜í™˜
        return result

    except Exception as e:
        logger.error(f"Search error: {str(e)}", exc_info=True)
        timings["error"] = time.time() - total_start
        logger.error(f"â±ï¸ ì—ëŸ¬ ë°œìƒ ì‹œì : {timings['error']:.3f}ì´ˆ")

        from fastapi.responses import JSONResponse

        return JSONResponse(
            status_code=500,
            content={
                "error": "ê²€ìƒ‰ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
                "detail": str(e),
                "timings": timings,
            },
        )
        raise

    finally:
        pass


# ìºì‹œ ê´€ë¦¬ ì—”ë“œí¬ì¸íŠ¸
@router.get("/cache/status")
async def get_cache_status():
    """ìºì‹œ ìƒíƒœ í™•ì¸"""
    valid_count = 0
    expired_count = 0
    current_time = datetime.now()

    for key, data in cache.items():
        if current_time < data["expires_at"]:
            valid_count += 1
        else:
            expired_count += 1

    return {
        "total_items": len(cache),
        "valid_items": valid_count,
        "expired_items": expired_count,
        "max_size": MAX_CACHE_SIZE,
        "ttl_seconds": CACHE_TTL_SECONDS,
        "memory_usage_mb": sum(len(str(v).encode("utf-8")) for v in cache.values())
        / (1024 * 1024),
    }


@router.delete("/cache/clear")
async def clear_cache():
    """ìºì‹œ ì´ˆê¸°í™”"""
    old_size = len(cache)
    cache.clear()
    logger.info(f"ğŸ—‘ï¸ ìºì‹œ ì´ˆê¸°í™”ë¨: {old_size}ê°œ í•­ëª© ì‚­ì œ")
    return {"cleared_items": old_size, "message": "ìºì‹œê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤."}


@router.delete("/cache/expired")
async def clear_expired_cache():
    """ë§Œë£Œëœ ìºì‹œ í•­ëª©ë§Œ ì‚­ì œ"""
    current_time = datetime.now()
    expired_keys = []

    for key, data in cache.items():
        if current_time >= data["expires_at"]:
            expired_keys.append(key)

    for key in expired_keys:
        del cache[key]

    logger.info(f"ğŸ—‘ï¸ ë§Œë£Œëœ ìºì‹œ {len(expired_keys)}ê°œ í•­ëª© ì‚­ì œ")
    return {
        "deleted_items": len(expired_keys),
        "message": f"{len(expired_keys)}ê°œì˜ ë§Œë£Œëœ í•­ëª©ì´ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.",
    }


# ì• í”Œë¦¬ì¼€ì´ì…˜ ì¢…ë£Œ ì‹œì—ë§Œ ì •ë¦¬
@router.on_event("shutdown")
async def shutdown_event():
    executor.shutdown(wait=True)
